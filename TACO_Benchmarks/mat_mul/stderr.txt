Registering autoscheduler...
JIT compiling shared runtime for x86-64-linux-avx-avx2-cuda-f16c-fma-jit-sse41
JIT compiling cuda for x86-64-linux-avx-avx2-cuda-f16c-fma-jit-sse41
Generator mat_mul has base_path ./bin/mat_mul_auto_schedule
Dropout seed = 1561557856
Node: output
  Symbolic region required: 
    output.x.min, output.x.max
    output.y.min, output.y.max
  Region computed: 
    output.x.min, output.x.max
    output.y.min, output.y.max
  Stage 0:
    x output.x.min output.x.max
    y output.y.min output.y.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   4
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   0
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   1
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      1 0 0 1
      Transpose:      1 0 0 1
      Broadcast:      1 0 0 1
      Slice:          1 0 0 1
  pointwise: 1 boundary condition: 0 wrapper: 1 input: 0 output: 1
Node: matrix_mul
  Symbolic region required: 
    matrix_mul.x.min, matrix_mul.x.max
    matrix_mul.y.min, matrix_mul.y.max
  Region computed: 
    matrix_mul.x.min, matrix_mul.x.max
    matrix_mul.y.min, matrix_mul.y.max
  Stage 0:
    x matrix_mul.x.min matrix_mul.x.max
    y matrix_mul.y.min matrix_mul.y.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   2
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   1
      Cast:       0
      Variable:   0
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 1
      Transpose:      0 0 0 1
      Broadcast:      0 0 0 1
      Slice:          0 0 0 1
  Stage 1:
    r4$x 0 1535
    x matrix_mul.x.min matrix_mul.x.max
    y matrix_mul.y.min matrix_mul.y.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   8
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   0
      Param:      0
      Add:        1
      Sub:        0
      Mod:        0
      Mul:        1
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   2
      SelfCall:   1
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
  pointwise: 0 boundary condition: 0 wrapper: 0 input: 0 output: 0
Node: input_b_im
  Symbolic region required: 
    input_b_im._0.min, input_b_im._0.max
    input_b_im._1.min, input_b_im._1.max
  Region computed: 
    input_b_im._0.min, input_b_im._0.max
    input_b_im._1.min, input_b_im._1.max
  Stage 0:
    _0 input_b_im._0.min input_b_im._0.max
    _1 input_b_im._1.min input_b_im._1.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   4
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   0
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  1
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 1 1
      Transpose:      0 0 1 1
      Broadcast:      0 0 1 1
      Slice:          0 0 1 1
  pointwise: 1 boundary condition: 0 wrapper: 1 input: 1 output: 0
Node: input_a_im
  Symbolic region required: 
    input_a_im._0.min, input_a_im._0.max
    input_a_im._1.min, input_a_im._1.max
  Region computed: 
    input_a_im._0.min, input_a_im._0.max
    input_a_im._1.min, input_a_im._1.max
  Stage 0:
    _0 input_a_im._0.min input_a_im._0.max
    _1 input_a_im._1.min input_a_im._1.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   4
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   0
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  1
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 1 1
      Transpose:      0 0 1 1
      Broadcast:      0 0 1 1
      Slice:          0 0 1 1
  pointwise: 1 boundary condition: 0 wrapper: 1 input: 1 output: 0
Edge: matrix_mul -> output
  Footprint: 
    Min 0: output.x.min
    Max 0: output.x.max
    Min 1: output.y.min
    Max 1: output.y.max
  Load Jacobians:
    [ 1   0  ]
    [ 0   1  ]

Edge: input_a_im -> matrix_mul.update(0)
  Footprint: 
    Min 0: matrix_mul.r4$x.min
    Max 0: matrix_mul.r4$x.max
    Min 1: matrix_mul.y.min
    Max 1: matrix_mul.y.max
  Load Jacobians:
    [ 1   0   0  ]
    [ 0   0   1  ]

Edge: input_b_im -> matrix_mul.update(0)
  Footprint: 
    Min 0: matrix_mul.x.min
    Max 0: matrix_mul.x.max
    Min 1: matrix_mul.r4$x.min
    Max 1: matrix_mul.r4$x.max
  Load Jacobians:
    [ 0   1   0  ]
    [ 1   0   0  ]


Pass 0 result:
State with cost 252.957443:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
output 24c 96c (0, 0) t gpu_block
 realize: matrix_mul [64c, 16c] with 2 stages
 matrix_mul 64c 16c (0, 0) t gpu_thread
  matrix_mul 1c 1c (0, 0) t gpu_serial
   matrix_mul 1vc 1c (0, 0) *
 matrix_mul 1c 64c 16c (1, 0) t gpu_thread
  matrix_mul 1536c 1c 1c (1, 0) t gpu_serial
   matrix_mul 1c 1vc 1c (1, 0) *
 output 64c 16c (0, 0) t gpu_thread
  output 1c 1c (0, 0) t gpu_serial
   output 1vc 1c (0, 0) *

Pass 1 result:
State with cost 120.943291:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
output 48c 48c (0, 0) t gpu_block
 realize: matrix_mul [32c, 32c] with 2 stages
 matrix_mul 32c 32c (0, 0) t gpu_thread
  matrix_mul 1c 1c (0, 0) t gpu_serial
   matrix_mul 1vc 1c (0, 0) *
 matrix_mul 1c 32c 32c (1, 0) t gpu_thread
  matrix_mul 1536c 1c 1c (1, 0) t gpu_serial
   matrix_mul 1c 1vc 1c (1, 0) *
 output 2c 1c (0, 0) t gpu_serial
  output 16c 32c (0, 0) t gpu_thread
   output 1c 1c (0, 0) t gpu_serial
    output 1vc 1c (0, 0) *

Pass 2 result:
State with cost 120.943291:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
output 48c 48c (0, 0) t gpu_block
 realize: matrix_mul [32c, 32c] with 2 stages
 matrix_mul 32c 32c (0, 0) t gpu_thread
  matrix_mul 1c 1c (0, 0) t gpu_serial
   matrix_mul 1vc 1c (0, 0) *
 matrix_mul 1c 32c 32c (1, 0) t gpu_thread
  matrix_mul 1536c 1c 1c (1, 0) t gpu_serial
   matrix_mul 1c 1vc 1c (1, 0) *
 output 2c 1c (0, 0) t gpu_serial
  output 16c 32c (0, 0) t gpu_thread
   output 1c 1c (0, 0) t gpu_serial
    output 1vc 1c (0, 0) *

Pass 3 result:
State with cost 229.383392:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
output 48c 48c (0, 0) t gpu_block
 realize: matrix_mul [32c, 32c] with 2 stages
 matrix_mul 16c 32c (0, 0) t gpu_thread
  matrix_mul 2c 1c (0, 0) t gpu_serial
   matrix_mul 1vc 1c (0, 0) *
 matrix_mul 1c 16c 32c (1, 0) t gpu_thread
  matrix_mul 1536c 2c 1c (1, 0) t gpu_serial
   matrix_mul 1c 1vc 1c (1, 0) *
 output 32c 32c (0, 0) t gpu_thread
  output 1c 1c (0, 0) t gpu_serial
   output 1vc 1c (0, 0) *

Pass 4 result:
State with cost 131.652878:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
output 48c 24c (0, 0) t gpu_block
 realize: matrix_mul [32c, 64c] with 2 stages
 matrix_mul 16c 64c (1, 1) t gpu_thread
  matrix_mul 2c 1c (1, 1) t gpu_serial
   matrix_mul 1c 1vc (1, 1) *
 matrix_mul 1c 16c 64c (2, 1) t gpu_thread
  matrix_mul 1536c 2c 1c (2, 1) t gpu_serial
   matrix_mul 1c 1c 1vc (2, 1) *
 output 1c 4c (0, 0) t gpu_serial
  output 16c 4c (0, 0) t gpu_thread
   output 2c 4c (0, 0) t gpu_serial
    output 1vc 1c (0, 0) *
Best cost: 120.943291
AutoSchedule.cpp:4446 ... AutoSchedule.cpp:4525 : 315.667251 ms
Cost evaluated this many times: 17365
** Optimal schedule:
Schedule features for output
    num_realizations:                      1.000000
    num_productions:                       1.000000
    points_computed_per_realization:       2359296.000000
    points_computed_per_production:        2359296.000000
    points_computed_total:                 2359296.000000
    points_computed_minimum:               2359296.000000
    innermost_loop_extent:                 1.000000
    innermost_pure_loop_extent:            1.000000
    unrolled_loop_extent:                  1.000000
    inner_parallelism:                     2304.000000
    outer_parallelism:                     1.000000
    bytes_at_realization:                  9437184.000000
    bytes_at_production:                   9437184.000000
    bytes_at_root:                         9437184.000000
    innermost_bytes_at_realization:        6144.000000
    innermost_bytes_at_production:         6144.000000
    innermost_bytes_at_root:               6144.000000
    inlined_calls:                         0.000000
    unique_bytes_read_per_realization:     4096.000000
    unique_lines_read_per_realization:     32.000000
    allocation_bytes_read_per_realization: 4096.000000
    working_set:                           4096.000000
    vector_size:                           1.000000
    native_vector_size:                    1.000000
    num_vectors:                           0.000000
    num_scalars:                           2359296.000000
    scalar_loads_per_vector:               0.000000
    vector_loads_per_vector:               1.000000
    scalar_loads_per_scalar:               1.000000
    bytes_at_task:                         4096.000000
    innermost_bytes_at_task:               128.000000
    unique_bytes_read_per_vector:          4.000000
    unique_lines_read_per_vector:          1.000000
    unique_bytes_read_per_task:            393216.000000
    unique_lines_read_per_task:            1568.000000
    working_set_at_task:                   4096.000000
    working_set_at_production:             9441280.000000
    working_set_at_realization:            9441280.000000
    working_set_at_root:                   9441280.000000
    num_warps:                             73728.000000
    block_occupancy:                       0.500000
    warp_lane_utilization:                 1.000000
    num_shared_mem_loads:                  73728.000000
    num_global_mem_loads:                  0.000000
    num_shared_mem_stores:                 0.000000
    num_global_mem_stores:                 589824.000000
Schedule features for matrix_mul
    num_realizations:                      2304.000000
    num_productions:                       2304.000000
    points_computed_per_realization:       1024.000000
    points_computed_per_production:        1024.000000
    points_computed_total:                 2359296.000000
    points_computed_minimum:               2359296.000000
    innermost_loop_extent:                 1.000000
    innermost_pure_loop_extent:            1.000000
    unrolled_loop_extent:                  1.000000
    inner_parallelism:                     1.000000
    outer_parallelism:                     2304.000000
    bytes_at_realization:                  4096.000000
    bytes_at_production:                   4096.000000
    bytes_at_root:                         9437184.000000
    innermost_bytes_at_realization:        128.000000
    innermost_bytes_at_production:         128.000000
    innermost_bytes_at_root:               6144.000000
    inlined_calls:                         0.000000
    unique_bytes_read_per_realization:     0.000000
    unique_lines_read_per_realization:     0.000000
    allocation_bytes_read_per_realization: 0.000000
    working_set:                           0.000000
    vector_size:                           1.000000
    native_vector_size:                    1.000000
    num_vectors:                           0.000000
    num_scalars:                           2359296.000000
    scalar_loads_per_vector:               0.000000
    vector_loads_per_vector:               0.000000
    scalar_loads_per_scalar:               0.000000
    bytes_at_task:                         4096.000000
    innermost_bytes_at_task:               128.000000
    unique_bytes_read_per_vector:          0.000000
    unique_lines_read_per_vector:          0.000000
    unique_bytes_read_per_task:            0.000000
    unique_lines_read_per_task:            0.000000
    working_set_at_task:                   4096.000000
    working_set_at_production:             4096.000000
    working_set_at_realization:            4096.000000
    working_set_at_root:                   9441280.000000
    num_warps:                             73728.000000
    block_occupancy:                       1.000000
    warp_lane_utilization:                 1.000000
    num_shared_mem_loads:                  0.000000
    num_global_mem_loads:                  0.000000
    num_shared_mem_stores:                 73728.000000
    num_global_mem_stores:                 0.000000
Schedule features for matrix_mul.update(0)
    num_realizations:                      2304.000000
    num_productions:                       2304.000000
    points_computed_per_realization:       1572864.000000
    points_computed_per_production:        1572864.000000
    points_computed_total:                 3623878656.000000
    points_computed_minimum:               3623878656.000000
    innermost_loop_extent:                 1536.000000
    innermost_pure_loop_extent:            1.000000
    unrolled_loop_extent:                  1.000000
    inner_parallelism:                     1.000000
    outer_parallelism:                     2304.000000
    bytes_at_realization:                  4096.000000
    bytes_at_production:                   4096.000000
    bytes_at_root:                         9437184.000000
    innermost_bytes_at_realization:        128.000000
    innermost_bytes_at_production:         128.000000
    innermost_bytes_at_root:               6144.000000
    inlined_calls:                         0.000000
    unique_bytes_read_per_realization:     397312.000000
    unique_lines_read_per_realization:     1600.000000
    allocation_bytes_read_per_realization: 18878464.000000
    working_set:                           0.000000
    vector_size:                           1.000000
    native_vector_size:                    1.000000
    num_vectors:                           0.000000
    num_scalars:                           3623878656.000000
    scalar_loads_per_vector:               1.000000
    vector_loads_per_vector:               2.000000
    scalar_loads_per_scalar:               3.000000
    bytes_at_task:                         4096.000000
    innermost_bytes_at_task:               128.000000
    unique_bytes_read_per_vector:          8.000000
    unique_lines_read_per_vector:          2.000000
    unique_bytes_read_per_task:            0.000000
    unique_lines_read_per_task:            0.000000
    working_set_at_task:                   4096.000000
    working_set_at_production:             4096.000000
    working_set_at_realization:            4096.000000
    working_set_at_root:                   9441280.000000
    num_warps:                             73728.000000
    block_occupancy:                       1.000000
    warp_lane_utilization:                 1.000000
    num_shared_mem_loads:                  0.000000
    num_global_mem_loads:                  663552.000000
    num_shared_mem_stores:                 73728.000000
    num_global_mem_stores:                 0.000000
State with cost 0.000000:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
output 48c 48c (0, 0) t gpu_block
 realize: matrix_mul [32c, 32c] with 2 stages
 matrix_mul 32c 32c (0, 0) t gpu_thread
  matrix_mul 1c 1c (0, 0) t gpu_serial
   matrix_mul 1vc 1c (0, 0) *
 matrix_mul 1c 32c 32c (1, 0) t gpu_thread
  matrix_mul 1536c 1c 1c (1, 0) t gpu_serial
   matrix_mul 1c 1vc 1c (1, 0) *
 output 2c 1c (0, 0) t gpu_serial
  output 16c 32c (0, 0) t gpu_thread
   output 1c 1c (0, 0) t gpu_serial
    output 1vc 1c (0, 0) *
Func output = get_pipeline().get_func(3);
Func matrix_mul = get_pipeline().get_func(2);
Var x(output.get_schedule().dims()[0].var), xi("xi"), xii("xii"), y(output.get_schedule().dims()[1].var), yi("yi");
RVar r4_x(matrix_mul.update(0).get_schedule().dims()[0].var);
output
    .split(x, x, xi, 32, TailStrategy::ShiftInwards)
    .split(y, y, yi, 32, TailStrategy::ShiftInwards)
    .split(xi, xi, xii, 16, TailStrategy::ShiftInwards)
    .compute_root()
    .reorder(xii, yi, xi, x, y)
    .gpu_blocks(y)
    .gpu_blocks(x)
    .split(xii, xii_serial_outer, xii, 16)
    .gpu_threads(xii)
    .split(yi, yi_serial_outer, yi, 32)
    .gpu_threads(yi);
matrix_mul.update(0)
    .reorder(r4_x, x, y)
    .split(x, x_serial_outer, x, 32)
    .gpu_threads(x)
    .split(y, y_serial_outer, y, 32)
    .gpu_threads(y);
matrix_mul
    .compute_at(output, x)
    .reorder(x, y)
    .split(x, x_serial_outer, x, 32)
    .gpu_threads(x)
    .split(y, y_serial_outer, y, 32)
    .gpu_threads(y);
Creating initial loop nests...
Injecting realization of { output }
Injecting realization of { matrix_mul }
Inlining input_b_im
Inlining input_a_im
Skipping injecting memoization...
Injecting tracing...
Adding checks for parameters
Computing bounds of each function's value
Adding checks for images
Performing computation bounds inference...
Removing extern loops...
Performing sliding window optimization...
Simplifying correlated differences...
Performing allocation bounds inference...
Removing code that depends on undef values...
Uniquifying variable names...
Simplifying...
Performing storage folding optimization...
Injecting debug_to_file calls...
Injecting prefetches...
Dynamically skipping stages...
Forking asynchronous producers...
Destructuring tuple-valued realizations...
Canonicalizing GPU var names...
Performing storage flattening...
Unpacking buffer arguments...
Skipping rewriting memoized allocations...
Selecting a GPU API for GPU loops...
Injecting host <-> dev buffer copies...
Selecting a GPU API for extern stages...
Simplifying...
Reduce prefetch dimension...
Lowering after reduce prefetch dimension:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input_b.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input_b"))
assert((reinterpret(uint64, input_a.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input_a"))
let input_a = _halide_buffer_get_host(input_a.buffer)
let input_a.type = _halide_buffer_get_type(input_a.buffer)
let input_a.dimensions = _halide_buffer_get_dimensions(input_a.buffer)
let input_a.min.0 = _halide_buffer_get_min(input_a.buffer, 0)
let input_a.extent.0 = _halide_buffer_get_extent(input_a.buffer, 0)
let input_a.stride.0 = _halide_buffer_get_stride(input_a.buffer, 0)
let input_a.min.1 = _halide_buffer_get_min(input_a.buffer, 1)
let input_a.extent.1 = _halide_buffer_get_extent(input_a.buffer, 1)
let input_a.stride.1 = _halide_buffer_get_stride(input_a.buffer, 1)
let input_b = _halide_buffer_get_host(input_b.buffer)
let input_b.type = _halide_buffer_get_type(input_b.buffer)
let input_b.dimensions = _halide_buffer_get_dimensions(input_b.buffer)
let input_b.min.0 = _halide_buffer_get_min(input_b.buffer, 0)
let input_b.extent.0 = _halide_buffer_get_extent(input_b.buffer, 0)
let input_b.stride.0 = _halide_buffer_get_stride(input_b.buffer, 0)
let input_b.min.1 = _halide_buffer_get_min(input_b.buffer, 1)
let input_b.extent.1 = _halide_buffer_get_extent(input_b.buffer, 1)
let input_b.stride.1 = _halide_buffer_get_stride(input_b.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.type = _halide_buffer_get_type(output.buffer)
let output.dimensions = _halide_buffer_get_dimensions(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
if (_halide_buffer_is_bounds_query(input_a.buffer)) {
  _halide_buffer_init(input_a.buffer, _halide_buffer_get_shape(input_a.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (_halide_buffer_is_bounds_query(input_b.buffer)) {
  _halide_buffer_init(input_b.buffer, _halide_buffer_get_shape(input_b.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (_halide_buffer_is_bounds_query(output.buffer)) {
  _halide_buffer_init(output.buffer, _halide_buffer_get_shape(output.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (!(_halide_buffer_is_bounds_query(output.buffer) || (_halide_buffer_is_bounds_query(input_a.buffer) || _halide_buffer_is_bounds_query(input_b.buffer)))) {
  assert((input_a.type == (uint32)73730), halide_error_bad_type("Input buffer input_a", input_a.type, (uint32)73730))
  assert((input_a.dimensions == 2), halide_error_bad_dimensions("Input buffer input_a", input_a.dimensions, 2))
  assert((input_b.type == (uint32)73730), halide_error_bad_type("Input buffer input_b", input_b.type, (uint32)73730))
  assert((input_b.dimensions == 2), halide_error_bad_dimensions("Input buffer input_b", input_b.dimensions, 2))
  assert((output.type == (uint32)73730), halide_error_bad_type("Output buffer output", output.type, (uint32)73730))
  assert((output.dimensions == 2), halide_error_bad_dimensions("Output buffer output", output.dimensions, 2))
  assert(((input_a.min.0 <= 0) && (1536 <= (input_a.extent.0 + input_a.min.0))), halide_error_access_out_of_bounds("Input buffer input_a", 0, 0, 1535, input_a.min.0, ((input_a.extent.0 + input_a.min.0) + -1)))
  assert((0 <= input_a.extent.0), halide_error_buffer_extents_negative("Input buffer input_a", 0, input_a.extent.0))
  assert(((input_a.min.1 <= 0) && (1536 <= (input_a.extent.1 + input_a.min.1))), halide_error_access_out_of_bounds("Input buffer input_a", 1, 0, 1535, input_a.min.1, ((input_a.extent.1 + input_a.min.1) + -1)))
  assert((0 <= input_a.extent.1), halide_error_buffer_extents_negative("Input buffer input_a", 1, input_a.extent.1))
  assert(((input_b.min.0 <= 0) && (1536 <= (input_b.extent.0 + input_b.min.0))), halide_error_access_out_of_bounds("Input buffer input_b", 0, 0, 1535, input_b.min.0, ((input_b.extent.0 + input_b.min.0) + -1)))
  assert((0 <= input_b.extent.0), halide_error_buffer_extents_negative("Input buffer input_b", 0, input_b.extent.0))
  assert(((input_b.min.1 <= 0) && (1536 <= (input_b.extent.1 + input_b.min.1))), halide_error_access_out_of_bounds("Input buffer input_b", 1, 0, 1535, input_b.min.1, ((input_b.extent.1 + input_b.min.1) + -1)))
  assert((0 <= input_b.extent.1), halide_error_buffer_extents_negative("Input buffer input_b", 1, input_b.extent.1))
  assert(((output.min.0 <= 0) && (1536 <= (output.extent.0 + output.min.0))), halide_error_access_out_of_bounds("Output buffer output", 0, 0, 1535, output.min.0, ((output.extent.0 + output.min.0) + -1)))
  assert((0 <= output.extent.0), halide_error_buffer_extents_negative("Output buffer output", 0, output.extent.0))
  assert(((output.min.1 <= 0) && (1536 <= (output.extent.1 + output.min.1))), halide_error_access_out_of_bounds("Output buffer output", 1, 0, 1535, output.min.1, ((output.extent.1 + output.min.1) + -1)))
  assert((0 <= output.extent.1), halide_error_buffer_extents_negative("Output buffer output", 1, output.extent.1))
  assert((input_a.stride.0 == 1), halide_error_constraint_violated("input_a.stride.0", input_a.stride.0, "1", 1))
  assert((input_b.stride.0 == 1), halide_error_constraint_violated("input_b.stride.0", input_b.stride.0, "1", 1))
  assert((output.stride.0 == 1), halide_error_constraint_violated("output.stride.0", output.stride.0, "1", 1))
  let input_a.total_extent.1 = (int64(input_a.extent.1)*int64(input_a.extent.0))
  let input_b.total_extent.1 = (int64(input_b.extent.1)*int64(input_b.extent.0))
  let output.total_extent.1 = (int64(output.extent.1)*int64(output.extent.0))
  assert((abs(int64(input_a.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_a", abs(int64(input_a.extent.0)), (uint64)2147483647))
  assert((abs((int64(input_a.extent.1)*int64(input_a.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_a", abs((int64(input_a.extent.1)*int64(input_a.stride.1))), (uint64)2147483647))
  assert((input_a.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("input_a", input_a.total_extent.1, (int64)2147483647))
  assert((abs(int64(input_b.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_b", abs(int64(input_b.extent.0)), (uint64)2147483647))
  assert((abs((int64(input_b.extent.1)*int64(input_b.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_b", abs((int64(input_b.extent.1)*int64(input_b.stride.1))), (uint64)2147483647))
  assert((input_b.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("input_b", input_b.total_extent.1, (int64)2147483647))
  assert((abs(int64(output.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("output", abs(int64(output.extent.0)), (uint64)2147483647))
  assert((abs((int64(output.extent.1)*int64(output.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("output", abs((int64(output.extent.1)*int64(output.stride.1))), (uint64)2147483647))
  assert((output.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("output", output.total_extent.1, (int64)2147483647))
  assert(((0 <= output.min.1) && ((output.extent.1 + output.min.1) <= 1536)), halide_error_explicit_bounds_too_small("y", "output", 0, 1535, output.min.1, ((output.extent.1 + output.min.1) + -1)))
  assert(((0 <= output.min.0) && ((output.extent.0 + output.min.0) <= 1536)), halide_error_explicit_bounds_too_small("x", "output", 0, 1535, output.min.0, ((output.extent.0 + output.min.0) + -1)))
  produce output {
    let halide_copy_to_device_result = halide_copy_to_device(input_a.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result == 0), halide_copy_to_device_result)
    let halide_copy_to_device_result$1 = halide_copy_to_device(input_b.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result$1 == 0), halide_copy_to_device_result$1)
    let halide_copy_to_device_result$2 = halide_copy_to_device(output.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result$2 == 0), halide_copy_to_device_result$2)
    gpu_block<CUDA> (output.s0.y.y.__block_id_y, 0, 48) {
      gpu_block<CUDA> (output.s0.x.x.__block_id_x, 0, 48) {
        allocate matrix_mul[float32 * 32 * 32]
        produce matrix_mul {
          gpu_thread<CUDA> (matrix_mul.s0.y.y.__thread_id_y, 0, 32) {
            if (likely((uint1)1)) {
              gpu_thread<CUDA> (matrix_mul.s0.x.x.__thread_id_x, 0, 32) {
                matrix_mul[((matrix_mul.s0.y.y.__thread_id_y*32) + matrix_mul.s0.x.x.__thread_id_x)] = 0.000000f
              }
            }
          }
          gpu_thread<CUDA> (matrix_mul.s1.y.y.__thread_id_y, 0, 32) {
            if (likely((uint1)1)) {
              gpu_thread<CUDA> (matrix_mul.s1.x.x.__thread_id_x, 0, 32) {
                for (matrix_mul.s1.r4$x, 0, 1536) {
                  matrix_mul[((matrix_mul.s1.y.y.__thread_id_y*32) + matrix_mul.s1.x.x.__thread_id_x)] = (matrix_mul[((matrix_mul.s1.y.y.__thread_id_y*32) + matrix_mul.s1.x.x.__thread_id_x)] + (input_a[(((((output.s0.y.y.__block_id_y*32) + matrix_mul.s1.y.y.__thread_id_y)*input_a.stride.1) + matrix_mul.s1.r4$x) - ((input_a.min.1*input_a.stride.1) + input_a.min.0))]*input_b[(((input_b.stride.1*matrix_mul.s1.r4$x) + ((output.s0.x.x.__block_id_x*32) + matrix_mul.s1.x.x.__thread_id_x)) - ((input_b.min.1*input_b.stride.1) + input_b.min.0))]))
                }
              }
            }
          }
        }
        consume matrix_mul {
          for (output.s0.x.xi.xi, 0, 2) {
            gpu_thread<CUDA> (output.s0.y.yi.yi.__thread_id_y, 0, 32) {
              gpu_thread<CUDA> (output.s0.x.xi.xii.xii.__thread_id_x, 0, 16) {
                output[((((output.s0.y.y.__block_id_y*32) + output.s0.y.yi.yi.__thread_id_y)*output.stride.1) + ((output.s0.x.x.__block_id_x*32) + ((output.s0.x.xi.xi*16) + output.s0.x.xi.xii.xii.__thread_id_x)))] = matrix_mul[((output.s0.y.yi.yi.__thread_id_y*32) + ((output.s0.x.xi.xi*16) + output.s0.x.xi.xii.xii.__thread_id_x))]
              }
            }
          }
        }
      }
    }
    _halide_buffer_set_device_dirty(output.buffer, (uint1)1)
  }
}

Simplifying correlated differences...
Unrolling...
Vectorizing...
Injecting per-block gpu synchronization...
Detecting vector interleavings...
Partitioning loops to simplify boundary conditions...
Trimming loops to the region over which they do something...
Injecting early frees...
Simplifying correlated differences...
Bounding small allocations...
Injecting warp shuffles...
Simplifying...
Lowering unsafe promises...
Lowering after final simplification:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input_b.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input_b"))
assert((reinterpret(uint64, input_a.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input_a"))
let input_a = _halide_buffer_get_host(input_a.buffer)
let input_a.type = _halide_buffer_get_type(input_a.buffer)
let input_a.dimensions = _halide_buffer_get_dimensions(input_a.buffer)
let input_a.min.0 = _halide_buffer_get_min(input_a.buffer, 0)
let input_a.extent.0 = _halide_buffer_get_extent(input_a.buffer, 0)
let input_a.stride.0 = _halide_buffer_get_stride(input_a.buffer, 0)
let input_a.min.1 = _halide_buffer_get_min(input_a.buffer, 1)
let input_a.extent.1 = _halide_buffer_get_extent(input_a.buffer, 1)
let input_a.stride.1 = _halide_buffer_get_stride(input_a.buffer, 1)
let input_b = _halide_buffer_get_host(input_b.buffer)
let input_b.type = _halide_buffer_get_type(input_b.buffer)
let input_b.dimensions = _halide_buffer_get_dimensions(input_b.buffer)
let input_b.min.0 = _halide_buffer_get_min(input_b.buffer, 0)
let input_b.extent.0 = _halide_buffer_get_extent(input_b.buffer, 0)
let input_b.stride.0 = _halide_buffer_get_stride(input_b.buffer, 0)
let input_b.min.1 = _halide_buffer_get_min(input_b.buffer, 1)
let input_b.extent.1 = _halide_buffer_get_extent(input_b.buffer, 1)
let input_b.stride.1 = _halide_buffer_get_stride(input_b.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.type = _halide_buffer_get_type(output.buffer)
let output.dimensions = _halide_buffer_get_dimensions(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
if (_halide_buffer_is_bounds_query(input_a.buffer)) {
  _halide_buffer_init(input_a.buffer, _halide_buffer_get_shape(input_a.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (_halide_buffer_is_bounds_query(input_b.buffer)) {
  _halide_buffer_init(input_b.buffer, _halide_buffer_get_shape(input_b.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (_halide_buffer_is_bounds_query(output.buffer)) {
  _halide_buffer_init(output.buffer, _halide_buffer_get_shape(output.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (!(_halide_buffer_is_bounds_query(output.buffer) || (_halide_buffer_is_bounds_query(input_a.buffer) || _halide_buffer_is_bounds_query(input_b.buffer)))) {
  assert((input_a.type == (uint32)73730), halide_error_bad_type("Input buffer input_a", input_a.type, (uint32)73730))
  assert((input_a.dimensions == 2), halide_error_bad_dimensions("Input buffer input_a", input_a.dimensions, 2))
  assert((input_b.type == (uint32)73730), halide_error_bad_type("Input buffer input_b", input_b.type, (uint32)73730))
  assert((input_b.dimensions == 2), halide_error_bad_dimensions("Input buffer input_b", input_b.dimensions, 2))
  assert((output.type == (uint32)73730), halide_error_bad_type("Output buffer output", output.type, (uint32)73730))
  assert((output.dimensions == 2), halide_error_bad_dimensions("Output buffer output", output.dimensions, 2))
  assert(((input_a.min.0 <= 0) && (1536 <= (input_a.extent.0 + input_a.min.0))), halide_error_access_out_of_bounds("Input buffer input_a", 0, 0, 1535, input_a.min.0, ((input_a.extent.0 + input_a.min.0) + -1)))
  assert((0 <= input_a.extent.0), halide_error_buffer_extents_negative("Input buffer input_a", 0, input_a.extent.0))
  assert(((input_a.min.1 <= 0) && (1536 <= (input_a.extent.1 + input_a.min.1))), halide_error_access_out_of_bounds("Input buffer input_a", 1, 0, 1535, input_a.min.1, ((input_a.extent.1 + input_a.min.1) + -1)))
  assert((0 <= input_a.extent.1), halide_error_buffer_extents_negative("Input buffer input_a", 1, input_a.extent.1))
  assert(((input_b.min.0 <= 0) && (1536 <= (input_b.extent.0 + input_b.min.0))), halide_error_access_out_of_bounds("Input buffer input_b", 0, 0, 1535, input_b.min.0, ((input_b.extent.0 + input_b.min.0) + -1)))
  assert((0 <= input_b.extent.0), halide_error_buffer_extents_negative("Input buffer input_b", 0, input_b.extent.0))
  assert(((input_b.min.1 <= 0) && (1536 <= (input_b.extent.1 + input_b.min.1))), halide_error_access_out_of_bounds("Input buffer input_b", 1, 0, 1535, input_b.min.1, ((input_b.extent.1 + input_b.min.1) + -1)))
  assert((0 <= input_b.extent.1), halide_error_buffer_extents_negative("Input buffer input_b", 1, input_b.extent.1))
  assert(((output.min.0 <= 0) && (1536 <= (output.extent.0 + output.min.0))), halide_error_access_out_of_bounds("Output buffer output", 0, 0, 1535, output.min.0, ((output.extent.0 + output.min.0) + -1)))
  assert((0 <= output.extent.0), halide_error_buffer_extents_negative("Output buffer output", 0, output.extent.0))
  assert(((output.min.1 <= 0) && (1536 <= (output.extent.1 + output.min.1))), halide_error_access_out_of_bounds("Output buffer output", 1, 0, 1535, output.min.1, ((output.extent.1 + output.min.1) + -1)))
  assert((0 <= output.extent.1), halide_error_buffer_extents_negative("Output buffer output", 1, output.extent.1))
  assert((input_a.stride.0 == 1), halide_error_constraint_violated("input_a.stride.0", input_a.stride.0, "1", 1))
  assert((input_b.stride.0 == 1), halide_error_constraint_violated("input_b.stride.0", input_b.stride.0, "1", 1))
  assert((output.stride.0 == 1), halide_error_constraint_violated("output.stride.0", output.stride.0, "1", 1))
  let input_a.total_extent.1 = (int64(input_a.extent.1)*int64(input_a.extent.0))
  let input_b.total_extent.1 = (int64(input_b.extent.1)*int64(input_b.extent.0))
  let output.total_extent.1 = (int64(output.extent.1)*int64(output.extent.0))
  assert((abs(int64(input_a.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_a", abs(int64(input_a.extent.0)), (uint64)2147483647))
  assert((abs((int64(input_a.extent.1)*int64(input_a.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_a", abs((int64(input_a.extent.1)*int64(input_a.stride.1))), (uint64)2147483647))
  assert((input_a.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("input_a", input_a.total_extent.1, (int64)2147483647))
  assert((abs(int64(input_b.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_b", abs(int64(input_b.extent.0)), (uint64)2147483647))
  assert((abs((int64(input_b.extent.1)*int64(input_b.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_b", abs((int64(input_b.extent.1)*int64(input_b.stride.1))), (uint64)2147483647))
  assert((input_b.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("input_b", input_b.total_extent.1, (int64)2147483647))
  assert((abs(int64(output.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("output", abs(int64(output.extent.0)), (uint64)2147483647))
  assert((abs((int64(output.extent.1)*int64(output.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("output", abs((int64(output.extent.1)*int64(output.stride.1))), (uint64)2147483647))
  assert((output.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("output", output.total_extent.1, (int64)2147483647))
  assert(((0 <= output.min.1) && ((output.extent.1 + output.min.1) <= 1536)), halide_error_explicit_bounds_too_small("y", "output", 0, 1535, output.min.1, ((output.extent.1 + output.min.1) + -1)))
  assert(((0 <= output.min.0) && ((output.extent.0 + output.min.0) <= 1536)), halide_error_explicit_bounds_too_small("x", "output", 0, 1535, output.min.0, ((output.extent.0 + output.min.0) + -1)))
  produce output {
    let halide_copy_to_device_result = halide_copy_to_device(input_a.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result == 0), halide_copy_to_device_result)
    let halide_copy_to_device_result$1 = halide_copy_to_device(input_b.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result$1 == 0), halide_copy_to_device_result$1)
    let halide_copy_to_device_result$2 = halide_copy_to_device(output.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result$2 == 0), halide_copy_to_device_result$2)
    let t59 = ((input_b.min.1*input_b.stride.1) + input_b.min.0)
    let t58 = ((input_a.min.1*input_a.stride.1) + input_a.min.0)
    gpu_block<CUDA> (output.s0.y.y.__block_id_y, 0, 48) {
      gpu_block<CUDA> (output.s0.x.x.__block_id_x, 0, 48) {
        allocate __shared[uint8 * 4096] in GPUShared
        gpu_thread<CUDA> (.__thread_id_y, 0, 32) {
          gpu_thread<CUDA> (.__thread_id_x, 0, 32) {
            produce matrix_mul {
              __shared[((.__thread_id_y*32) + .__thread_id_x)] = 0.000000f
              gpu_thread_barrier()
              let t53 = ((((output.s0.y.y.__block_id_y*32) + .__thread_id_y)*input_a.stride.1) - t58)
              let t54.s = ((output.s0.x.x.__block_id_x*32) - t59)
              let t60 = ((.__thread_id_y*32) + .__thread_id_x)
              let t61 = (.__thread_id_x + t54.s)
              for (matrix_mul.s1.r4$x, 0, 1536) {
                __shared[t60] = (__shared[t60] + (input_a[(matrix_mul.s1.r4$x + t53)]*input_b[((input_b.stride.1*matrix_mul.s1.r4$x) + t61)]))
              }
            }
            gpu_thread_barrier()
            consume matrix_mul {
              let t55 = (.__thread_id_x < 16)
              let t57.s = ((output.s0.x.x.__block_id_x*32) + (((output.s0.y.y.__block_id_y*32) + .__thread_id_y)*output.stride.1))
              let t62 = ((.__thread_id_y*32) + .__thread_id_x)
              let t63 = (.__thread_id_x + t57.s)
              for (output.s0.x.xi.xi, 0, 2) {
                if (t55) {
                  output[((output.s0.x.xi.xi*16) + t63)] = __shared[((output.s0.x.xi.xi*16) + t62)]
                }
              }
            }
          }
        }
        free __shared
      }
    }
    _halide_buffer_set_device_dirty(output.buffer, (uint1)1)
  }
}


Skipping Hexagon offload...
Constructing CUDA device codegen
Target triple of initial module: x86_64--linux-gnu
Generating llvm bitcode...
Generating llvm bitcode prolog for function mat_mul_auto_schedule...
Generating llvm bitcode for function mat_mul_auto_schedule...
Generating llvm bitcode for kernel...
PTX kernel:
//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_35
.address_size 64

	// .globl	kernel_output_s0_y_y___block_id_y // -- Begin function kernel_output_s0_y_y___block_id_y
                                        // @kernel_output_s0_y_y___block_id_y
.visible .entry kernel_output_s0_y_y___block_id_y(
	.param .u32 kernel_output_s0_y_y___block_id_y_param_0,
	.param .u32 kernel_output_s0_y_y___block_id_y_param_1,
	.param .u32 kernel_output_s0_y_y___block_id_y_param_2,
	.param .u32 kernel_output_s0_y_y___block_id_y_param_3,
	.param .u32 kernel_output_s0_y_y___block_id_y_param_4,
	.param .u64 kernel_output_s0_y_y___block_id_y_param_5,
	.param .u64 kernel_output_s0_y_y___block_id_y_param_6,
	.param .u64 kernel_output_s0_y_y___block_id_y_param_7
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<22>;
	.reg .b32 	%r<64>;
	.reg .b64 	%rd<24>;

// %bb.0:                               // %entry
	ld.param.u32 	%r26, [kernel_output_s0_y_y___block_id_y_param_2];
	ld.param.u32 	%r28, [kernel_output_s0_y_y___block_id_y_param_0];
	ld.param.u64 	%rd6, [kernel_output_s0_y_y___block_id_y_param_7];
	cvta.to.global.u64 	%rd1, %rd6;
	ld.param.u32 	%r29, [kernel_output_s0_y_y___block_id_y_param_1];
	ld.param.u64 	%rd7, [kernel_output_s0_y_y___block_id_y_param_6];
	cvta.to.global.u64 	%rd2, %rd7;
	ld.param.u64 	%rd8, [kernel_output_s0_y_y___block_id_y_param_5];
	cvta.to.global.u64 	%rd3, %rd8;
	ld.param.u32 	%r30, [kernel_output_s0_y_y___block_id_y_param_3];
	ld.param.u32 	%r31, [kernel_output_s0_y_y___block_id_y_param_4];
	mov.u32 	%r32, %ctaid.y;
	mov.u32 	%r33, %ctaid.x;
	mov.u32 	%r34, %tid.y;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r35, %r34, 5;
	add.s32 	%r36, %r35, %r1;
	mul.wide.u32 	%rd4, %r36, 4;
	mov.u32 	%r63, 0;
	st.shared.u32 	[%rd4], %r63;
	bar.sync 	0;
	shl.b32 	%r37, %r32, 5;
	add.s32 	%r2, %r34, %r37;
	mul.lo.s32 	%r38, %r2, %r28;
	sub.s32 	%r3, %r38, %r30;
	shl.b32 	%r4, %r33, 5;
	ld.shared.f32 	%f21, [%rd4];
	add.s32 	%r39, %r29, %r4;
	sub.s32 	%r62, %r39, %r31;
	mul.lo.s32 	%r6, %r29, 6;
	shl.b32 	%r40, %r29, 1;
	add.s32 	%r41, %r4, %r40;
	sub.s32 	%r61, %r41, %r31;
	mad.lo.s32 	%r42, %r29, 3, %r4;
	sub.s32 	%r60, %r42, %r31;
	shl.b32 	%r43, %r29, 2;
	add.s32 	%r44, %r4, %r43;
	sub.s32 	%r59, %r44, %r31;
	mad.lo.s32 	%r45, %r29, 5, %r4;
	sub.s32 	%r58, %r45, %r31;
	sub.s32 	%r57, %r4, %r31;
LBB0_1:                                 // %"for matrix_mul.s1.r4$x"
                                        // =>This Inner Loop Header: Depth=1
	add.s32 	%r46, %r3, %r63;
	mul.wide.s32 	%rd9, %r46, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.nc.f32 	%f4, [%rd10];
	add.s32 	%r47, %r1, %r57;
	mul.wide.s32 	%rd11, %r47, 4;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.nc.f32 	%f5, [%rd12];
	fma.rn.ftz.f32 	%f6, %f4, %f5, %f21;
	ld.global.nc.f32 	%f7, [%rd10+4];
	add.s32 	%r48, %r1, %r62;
	mul.wide.s32 	%rd13, %r48, 4;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.nc.f32 	%f8, [%rd14];
	fma.rn.ftz.f32 	%f9, %f7, %f8, %f6;
	ld.global.nc.f32 	%f10, [%rd10+8];
	add.s32 	%r49, %r1, %r61;
	mul.wide.s32 	%rd15, %r49, 4;
	add.s64 	%rd16, %rd2, %rd15;
	ld.global.nc.f32 	%f11, [%rd16];
	fma.rn.ftz.f32 	%f12, %f10, %f11, %f9;
	ld.global.nc.f32 	%f13, [%rd10+12];
	add.s32 	%r50, %r1, %r60;
	mul.wide.s32 	%rd17, %r50, 4;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.f32 	%f14, [%rd18];
	fma.rn.ftz.f32 	%f15, %f13, %f14, %f12;
	ld.global.nc.f32 	%f16, [%rd10+16];
	add.s32 	%r51, %r1, %r59;
	mul.wide.s32 	%rd19, %r51, 4;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.nc.f32 	%f17, [%rd20];
	fma.rn.ftz.f32 	%f18, %f16, %f17, %f15;
	ld.global.nc.f32 	%f19, [%rd10+20];
	add.s32 	%r52, %r1, %r58;
	mul.wide.s32 	%rd21, %r52, 4;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.nc.f32 	%f20, [%rd22];
	fma.rn.ftz.f32 	%f21, %f19, %f20, %f18;
	add.s32 	%r63, %r63, 6;
	add.s32 	%r62, %r62, %r6;
	add.s32 	%r61, %r61, %r6;
	add.s32 	%r60, %r60, %r6;
	add.s32 	%r59, %r59, %r6;
	add.s32 	%r58, %r58, %r6;
	add.s32 	%r57, %r57, %r6;
	setp.ne.s32 	%p1, %r63, 1536;
	@%p1 bra 	LBB0_1;
// %bb.2:                               // %"end for matrix_mul.s1.r4$x"
	st.shared.f32 	[%rd4], %f21;
	bar.sync 	0;
	setp.gt.u32 	%p2, %r1, 15;
	@%p2 bra 	LBB0_4;
// %bb.3:                               // %"for output.s0.x.xi.xi.us.preheader"
	add.s32 	%r53, %r1, %r4;
	mad.lo.s32 	%r54, %r2, %r26, %r53;
	mul.wide.s32 	%rd23, %r54, 4;
	add.s64 	%rd5, %rd1, %rd23;
	ld.shared.u32 	%r55, [%rd4];
	st.global.u32 	[%rd5], %r55;
	ld.shared.u32 	%r56, [%rd4+64];
	st.global.u32 	[%rd5+64], %r56;
LBB0_4:                                 // %"end for output.s0.x.xi.xi"
	ret;
                                        // -- End function
}


add_temp_object_file: /tmp/AzckBf/mat_mul_auto_schedule.a.o
Module.compile(): temporary object_name /tmp/AzckBf/mat_mul_auto_schedule.a.o
emit_file.Compiling to native code...
Module.compile(): static_library_name ./bin/mat_mul_auto_schedule.a
file_unlink: /tmp/AzckBf/mat_mul_auto_schedule.a.o
dir_rmdir: /tmp/AzckBf
Module.compile(): c_header_name ./bin/mat_mul_auto_schedule.h
Module.compile(): registration_name ./bin/mat_mul_auto_schedule.registration.cpp
