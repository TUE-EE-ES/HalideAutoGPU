Registering autoscheduler...
JIT compiling shared runtime for x86-64-linux-avx-avx2-cuda-f16c-fma-jit-sse41
JIT compiling cuda for x86-64-linux-avx-avx2-cuda-f16c-fma-jit-sse41
Generator mat_mul has base_path ./bin/mat_mul_classic_auto_schedule
Dropout seed = 1561557858
Node: output
  Symbolic region required: 
    output.x.min, output.x.max
    output.y.min, output.y.max
  Region computed: 
    output.x.min, output.x.max
    output.y.min, output.y.max
  Stage 0:
    x output.x.min output.x.max
    y output.y.min output.y.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   4
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   0
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   1
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      1 0 0 1
      Transpose:      1 0 0 1
      Broadcast:      1 0 0 1
      Slice:          1 0 0 1
  pointwise: 1 boundary condition: 0 wrapper: 1 input: 0 output: 1
Node: matrix_mul
  Symbolic region required: 
    matrix_mul.x.min, matrix_mul.x.max
    matrix_mul.y.min, matrix_mul.y.max
  Region computed: 
    matrix_mul.x.min, matrix_mul.x.max
    matrix_mul.y.min, matrix_mul.y.max
  Stage 0:
    x matrix_mul.x.min matrix_mul.x.max
    y matrix_mul.y.min matrix_mul.y.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   2
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   1
      Cast:       0
      Variable:   0
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 1
      Transpose:      0 0 0 1
      Broadcast:      0 0 0 1
      Slice:          0 0 0 1
  Stage 1:
    r4$x 0 1535
    x matrix_mul.x.min matrix_mul.x.max
    y matrix_mul.y.min matrix_mul.y.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   8
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   0
      Param:      0
      Add:        1
      Sub:        0
      Mod:        0
      Mul:        1
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   2
      SelfCall:   1
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
  pointwise: 0 boundary condition: 0 wrapper: 0 input: 0 output: 0
Node: input_b_im
  Symbolic region required: 
    input_b_im._0.min, input_b_im._0.max
    input_b_im._1.min, input_b_im._1.max
  Region computed: 
    input_b_im._0.min, input_b_im._0.max
    input_b_im._1.min, input_b_im._1.max
  Stage 0:
    _0 input_b_im._0.min input_b_im._0.max
    _1 input_b_im._1.min input_b_im._1.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   4
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   0
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  1
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 1 1
      Transpose:      0 0 1 1
      Broadcast:      0 0 1 1
      Slice:          0 0 1 1
  pointwise: 1 boundary condition: 0 wrapper: 1 input: 1 output: 0
Node: input_a_im
  Symbolic region required: 
    input_a_im._0.min, input_a_im._0.max
    input_a_im._1.min, input_a_im._1.max
  Region computed: 
    input_a_im._0.min, input_a_im._0.max
    input_a_im._1.min, input_a_im._1.max
  Stage 0:
    _0 input_a_im._0.min input_a_im._0.max
    _1 input_a_im._1.min input_a_im._1.max
    Featurization for type UInt32
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   4
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  0
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 0 0
      Transpose:      0 0 0 0
      Broadcast:      0 0 0 0
      Slice:          0 0 0 0
    Featurization for type Float
     Op histogram:
      Constant:   0
      Cast:       0
      Variable:   0
      Param:      0
      Add:        0
      Sub:        0
      Mod:        0
      Mul:        0
      Div:        0
      Min:        0
      Max:        0
      EQ:         0
      NE:         0
      LT:         0
      LE:         0
      And:        0
      Or:         0
      Not:        0
      Select:     0
      ImageCall:  1
      FuncCall:   0
      SelfCall:   0
      ExternCall: 0
      Let:        0
     Memory access patterns. Columns are calls to other Funcs, self-calls, input image access, and stores
      Pointwise:      0 0 1 1
      Transpose:      0 0 1 1
      Broadcast:      0 0 1 1
      Slice:          0 0 1 1
  pointwise: 1 boundary condition: 0 wrapper: 1 input: 1 output: 0
Edge: matrix_mul -> output
  Footprint: 
    Min 0: output.x.min
    Max 0: output.x.max
    Min 1: output.y.min
    Max 1: output.y.max
  Load Jacobians:
    [ 1   0  ]
    [ 0   1  ]

Edge: input_a_im -> matrix_mul.update(0)
  Footprint: 
    Min 0: matrix_mul.r4$x.min
    Max 0: matrix_mul.r4$x.max
    Min 1: matrix_mul.y.min
    Max 1: matrix_mul.y.max
  Load Jacobians:
    [ 1   0   0  ]
    [ 0   0   1  ]

Edge: input_b_im -> matrix_mul.update(0)
  Footprint: 
    Min 0: matrix_mul.x.min
    Max 0: matrix_mul.x.max
    Min 1: matrix_mul.r4$x.min
    Max 1: matrix_mul.r4$x.max
  Load Jacobians:
    [ 0   1   0  ]
    [ 1   0   0  ]


Pass 0 result:
State with cost 150.570633:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
realize: matrix_mul [1536c, 1536c] with 2 stages
matrix_mul 3c 24c (1, 1) t gpu_block
 matrix_mul 64c 16c (1, 1) t gpu_thread
  matrix_mul 8c 4c (1, 1) t gpu_serial
   matrix_mul 1c 1vc (1, 1) *
matrix_mul 1c 3c 24c (2, 1) t gpu_block
 matrix_mul 1c 64c 16c (2, 1) t gpu_thread
  matrix_mul 1536c 8c 4c (2, 1) t gpu_serial
   matrix_mul 1c 1c 1vc (2, 1) *
output 96c 12c (0, 0) t gpu_block
 output 16c 64c (0, 0) t gpu_thread
  output 1c 2c (0, 0) t gpu_serial
   output 1vc 1c (0, 0) *

Pass 1 result:
State with cost 150.570633:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
realize: matrix_mul [1536c, 1536c] with 2 stages
matrix_mul 3c 24c (1, 1) t gpu_block
 matrix_mul 64c 16c (1, 1) t gpu_thread
  matrix_mul 8c 4c (1, 1) t gpu_serial
   matrix_mul 1c 1vc (1, 1) *
matrix_mul 1c 3c 24c (2, 1) t gpu_block
 matrix_mul 1c 64c 16c (2, 1) t gpu_thread
  matrix_mul 1536c 8c 4c (2, 1) t gpu_serial
   matrix_mul 1c 1c 1vc (2, 1) *
output 96c 12c (0, 0) t gpu_block
 output 16c 64c (0, 0) t gpu_thread
  output 1c 2c (0, 0) t gpu_serial
   output 1vc 1c (0, 0) *

Pass 2 result:
State with cost 150.570633:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
realize: matrix_mul [1536c, 1536c] with 2 stages
matrix_mul 3c 24c (1, 1) t gpu_block
 matrix_mul 64c 16c (1, 1) t gpu_thread
  matrix_mul 8c 4c (1, 1) t gpu_serial
   matrix_mul 1c 1vc (1, 1) *
matrix_mul 1c 3c 24c (2, 1) t gpu_block
 matrix_mul 1c 64c 16c (2, 1) t gpu_thread
  matrix_mul 1536c 8c 4c (2, 1) t gpu_serial
   matrix_mul 1c 1c 1vc (2, 1) *
output 96c 12c (0, 0) t gpu_block
 output 16c 64c (0, 0) t gpu_thread
  output 1c 2c (0, 0) t gpu_serial
   output 1vc 1c (0, 0) *

Pass 3 result:
State with cost 150.570633:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
realize: matrix_mul [1536c, 1536c] with 2 stages
matrix_mul 3c 24c (1, 1) t gpu_block
 matrix_mul 64c 16c (1, 1) t gpu_thread
  matrix_mul 8c 4c (1, 1) t gpu_serial
   matrix_mul 1c 1vc (1, 1) *
matrix_mul 1c 3c 24c (2, 1) t gpu_block
 matrix_mul 1c 64c 16c (2, 1) t gpu_thread
  matrix_mul 1536c 8c 4c (2, 1) t gpu_serial
   matrix_mul 1c 1c 1vc (2, 1) *
output 96c 12c (0, 0) t gpu_block
 output 16c 64c (0, 0) t gpu_thread
  output 1c 2c (0, 0) t gpu_serial
   output 1vc 1c (0, 0) *

Pass 4 result:
State with cost 150.570633:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
realize: matrix_mul [1536c, 1536c] with 2 stages
matrix_mul 3c 24c (1, 1) t gpu_block
 matrix_mul 64c 16c (1, 1) t gpu_thread
  matrix_mul 8c 4c (1, 1) t gpu_serial
   matrix_mul 1c 1vc (1, 1) *
matrix_mul 1c 3c 24c (2, 1) t gpu_block
 matrix_mul 1c 64c 16c (2, 1) t gpu_thread
  matrix_mul 1536c 8c 4c (2, 1) t gpu_serial
   matrix_mul 1c 1c 1vc (2, 1) *
output 96c 12c (0, 0) t gpu_block
 output 16c 64c (0, 0) t gpu_thread
  output 1c 2c (0, 0) t gpu_serial
   output 1vc 1c (0, 0) *
Best cost: 150.570633
AutoSchedule.cpp:4446 ... AutoSchedule.cpp:4525 : 779.565989 ms
Cost evaluated this many times: 47845
** Optimal schedule:
Schedule features for output
    num_realizations:                      1.000000
    num_productions:                       1.000000
    points_computed_per_realization:       2359296.000000
    points_computed_per_production:        2359296.000000
    points_computed_total:                 2359296.000000
    points_computed_minimum:               2359296.000000
    innermost_loop_extent:                 2.000000
    innermost_pure_loop_extent:            2.000000
    unrolled_loop_extent:                  2.000000
    inner_parallelism:                     1152.000000
    outer_parallelism:                     1.000000
    bytes_at_realization:                  9437184.000000
    bytes_at_production:                   9437184.000000
    bytes_at_root:                         9437184.000000
    innermost_bytes_at_realization:        6144.000000
    innermost_bytes_at_production:         6144.000000
    innermost_bytes_at_root:               6144.000000
    inlined_calls:                         0.000000
    unique_bytes_read_per_realization:     9437184.000000
    unique_lines_read_per_realization:     1536.000000
    allocation_bytes_read_per_realization: 9437184.000000
    working_set:                           0.000000
    vector_size:                           1.000000
    native_vector_size:                    1.000000
    num_vectors:                           0.000000
    num_scalars:                           2359296.000000
    scalar_loads_per_vector:               1.000000
    vector_loads_per_vector:               0.000000
    scalar_loads_per_scalar:               1.000000
    bytes_at_task:                         8192.000000
    innermost_bytes_at_task:               64.000000
    unique_bytes_read_per_vector:          4.000000
    unique_lines_read_per_vector:          1.000000
    unique_bytes_read_per_task:            8192.000000
    unique_lines_read_per_task:            16.000000
    working_set_at_task:                   0.000000
    working_set_at_production:             18874368.000000
    working_set_at_realization:            18874368.000000
    working_set_at_root:                   18874368.000000
    num_warps:                             36864.000000
    block_occupancy:                       1.000000
    warp_lane_utilization:                 1.000000
    num_shared_mem_loads:                  0.000000
    num_global_mem_loads:                  294912.000000
    num_shared_mem_stores:                 0.000000
    num_global_mem_stores:                 294912.000000
Schedule features for matrix_mul
    num_realizations:                      1.000000
    num_productions:                       1.000000
    points_computed_per_realization:       2359296.000000
    points_computed_per_production:        2359296.000000
    points_computed_total:                 2359296.000000
    points_computed_minimum:               2359296.000000
    innermost_loop_extent:                 32.000000
    innermost_pure_loop_extent:            32.000000
    unrolled_loop_extent:                  1.000000
    inner_parallelism:                     72.000000
    outer_parallelism:                     1.000000
    bytes_at_realization:                  9437184.000000
    bytes_at_production:                   9437184.000000
    bytes_at_root:                         9437184.000000
    innermost_bytes_at_realization:        6144.000000
    innermost_bytes_at_production:         6144.000000
    innermost_bytes_at_root:               6144.000000
    inlined_calls:                         0.000000
    unique_bytes_read_per_realization:     0.000000
    unique_lines_read_per_realization:     0.000000
    allocation_bytes_read_per_realization: 0.000000
    working_set:                           0.000000
    vector_size:                           1.000000
    native_vector_size:                    1.000000
    num_vectors:                           0.000000
    num_scalars:                           2359296.000000
    scalar_loads_per_vector:               0.000000
    vector_loads_per_vector:               0.000000
    scalar_loads_per_scalar:               0.000000
    bytes_at_task:                         131072.000000
    innermost_bytes_at_task:               256.000000
    unique_bytes_read_per_vector:          0.000000
    unique_lines_read_per_vector:          0.000000
    unique_bytes_read_per_task:            0.000000
    unique_lines_read_per_task:            0.000000
    working_set_at_task:                   0.000000
    working_set_at_production:             18874368.000000
    working_set_at_realization:            18874368.000000
    working_set_at_root:                   18874368.000000
    num_warps:                             2304.000000
    block_occupancy:                       1.000000
    warp_lane_utilization:                 1.000000
    num_shared_mem_loads:                  0.000000
    num_global_mem_loads:                  0.000000
    num_shared_mem_stores:                 0.000000
    num_global_mem_stores:                 73728.000000
Schedule features for matrix_mul.update(0)
    num_realizations:                      1.000000
    num_productions:                       1.000000
    points_computed_per_realization:       3623878656.000000
    points_computed_per_production:        3623878656.000000
    points_computed_total:                 3623878656.000000
    points_computed_minimum:               3623878656.000000
    innermost_loop_extent:                 49152.000000
    innermost_pure_loop_extent:            32.000000
    unrolled_loop_extent:                  1.000000
    inner_parallelism:                     72.000000
    outer_parallelism:                     1.000000
    bytes_at_realization:                  9437184.000000
    bytes_at_production:                   9437184.000000
    bytes_at_root:                         9437184.000000
    innermost_bytes_at_realization:        6144.000000
    innermost_bytes_at_production:         6144.000000
    innermost_bytes_at_root:               6144.000000
    inlined_calls:                         0.000000
    unique_bytes_read_per_realization:     28311552.000000
    unique_lines_read_per_realization:     4608.000000
    allocation_bytes_read_per_realization: 28311552.000000
    working_set:                           0.000000
    vector_size:                           1.000000
    native_vector_size:                    1.000000
    num_vectors:                           0.000000
    num_scalars:                           3623878656.000000
    scalar_loads_per_vector:               2.000000
    vector_loads_per_vector:               1.000000
    scalar_loads_per_scalar:               3.000000
    bytes_at_task:                         131072.000000
    innermost_bytes_at_task:               256.000000
    unique_bytes_read_per_vector:          8.000000
    unique_lines_read_per_vector:          2.000000
    unique_bytes_read_per_task:            3538944.000000
    unique_lines_read_per_task:            1600.000000
    working_set_at_task:                   0.000000
    working_set_at_production:             18874368.000000
    working_set_at_realization:            18874368.000000
    working_set_at_root:                   18874368.000000
    num_warps:                             2304.000000
    block_occupancy:                       1.000000
    warp_lane_utilization:                 1.000000
    num_shared_mem_loads:                  0.000000
    num_global_mem_loads:                  76032.000000
    num_shared_mem_stores:                 0.000000
    num_global_mem_stores:                 73728.000000
State with cost 0.000000:
 gpu_none
realize: output [1536c, 1536c] with 1 stages
realize: matrix_mul [1536c, 1536c] with 2 stages
matrix_mul 3c 24c (1, 1) t gpu_block
 matrix_mul 64c 16c (1, 1) t gpu_thread
  matrix_mul 8c 4c (1, 1) t gpu_serial
   matrix_mul 1c 1vc (1, 1) *
matrix_mul 1c 3c 24c (2, 1) t gpu_block
 matrix_mul 1c 64c 16c (2, 1) t gpu_thread
  matrix_mul 1536c 8c 4c (2, 1) t gpu_serial
   matrix_mul 1c 1c 1vc (2, 1) *
output 96c 12c (0, 0) t gpu_block
 output 16c 64c (0, 0) t gpu_thread
  output 1c 2c (0, 0) t gpu_serial
   output 1vc 1c (0, 0) *
Func output = get_pipeline().get_func(3);
Func matrix_mul = get_pipeline().get_func(2);
Var x(output.get_schedule().dims()[0].var), xi("xi"), xii("xii"), y(output.get_schedule().dims()[1].var), yi("yi"), yii("yii");
RVar r4_x(matrix_mul.update(0).get_schedule().dims()[0].var);
output
    .split(x, x, xi, 16, TailStrategy::ShiftInwards)
    .split(y, y, yi, 128, TailStrategy::ShiftInwards)
    .split(yi, yi, yii, 2, TailStrategy::ShiftInwards)
    .unroll(yii)
    .compute_root()
    .reorder(yii, xi, yi, x, y)
    .gpu_blocks(y)
    .gpu_blocks(x)
    .split(xi, xi_serial_outer, xi, 16)
    .gpu_threads(xi)
    .split(yi, yi_serial_outer, yi, 64)
    .gpu_threads(yi);
matrix_mul.update(0)
    .split(y, y, yi, 64, TailStrategy::GuardWithIf)
    .split(x, x, xi, 512, TailStrategy::GuardWithIf)
    .split(yi, yi, yii, 4, TailStrategy::GuardWithIf)
    .split(xi, xi, xii, 8, TailStrategy::GuardWithIf)
    .reorder(r4_x, yii, xii, yi, xi, y, x)
    .gpu_blocks(x)
    .gpu_blocks(y)
    .split(yi, yi_serial_outer, yi, 16)
    .gpu_threads(yi)
    .split(xi, xi_serial_outer, xi, 64)
    .gpu_threads(xi);
matrix_mul
    .split(y, y, yi, 64, TailStrategy::ShiftInwards)
    .split(x, x, xi, 512, TailStrategy::ShiftInwards)
    .split(yi, yi, yii, 4, TailStrategy::ShiftInwards)
    .split(xi, xi, xii, 8, TailStrategy::ShiftInwards)
    .compute_root()
    .reorder(yii, xii, yi, xi, y, x)
    .gpu_blocks(x)
    .gpu_blocks(y)
    .reorder_storage(y, x)
    .split(yi, yi_serial_outer, yi, 16)
    .gpu_threads(yi)
    .split(xi, xi_serial_outer, xi, 64)
    .gpu_threads(xi);
Creating initial loop nests...
Injecting realization of { output }
Injecting realization of { matrix_mul }
Inlining input_b_im
Inlining input_a_im
Skipping injecting memoization...
Injecting tracing...
Adding checks for parameters
Computing bounds of each function's value
Adding checks for images
Performing computation bounds inference...
Removing extern loops...
Performing sliding window optimization...
Simplifying correlated differences...
Performing allocation bounds inference...
Removing code that depends on undef values...
Uniquifying variable names...
Simplifying...
Performing storage folding optimization...
Injecting debug_to_file calls...
Injecting prefetches...
Dynamically skipping stages...
Forking asynchronous producers...
Destructuring tuple-valued realizations...
Canonicalizing GPU var names...
Performing storage flattening...
Unpacking buffer arguments...
Skipping rewriting memoized allocations...
Selecting a GPU API for GPU loops...
Injecting host <-> dev buffer copies...
Selecting a GPU API for extern stages...
Simplifying...
Reduce prefetch dimension...
Lowering after reduce prefetch dimension:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input_b.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input_b"))
assert((reinterpret(uint64, input_a.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input_a"))
let input_a = _halide_buffer_get_host(input_a.buffer)
let input_a.type = _halide_buffer_get_type(input_a.buffer)
let input_a.dimensions = _halide_buffer_get_dimensions(input_a.buffer)
let input_a.min.0 = _halide_buffer_get_min(input_a.buffer, 0)
let input_a.extent.0 = _halide_buffer_get_extent(input_a.buffer, 0)
let input_a.stride.0 = _halide_buffer_get_stride(input_a.buffer, 0)
let input_a.min.1 = _halide_buffer_get_min(input_a.buffer, 1)
let input_a.extent.1 = _halide_buffer_get_extent(input_a.buffer, 1)
let input_a.stride.1 = _halide_buffer_get_stride(input_a.buffer, 1)
let input_b = _halide_buffer_get_host(input_b.buffer)
let input_b.type = _halide_buffer_get_type(input_b.buffer)
let input_b.dimensions = _halide_buffer_get_dimensions(input_b.buffer)
let input_b.min.0 = _halide_buffer_get_min(input_b.buffer, 0)
let input_b.extent.0 = _halide_buffer_get_extent(input_b.buffer, 0)
let input_b.stride.0 = _halide_buffer_get_stride(input_b.buffer, 0)
let input_b.min.1 = _halide_buffer_get_min(input_b.buffer, 1)
let input_b.extent.1 = _halide_buffer_get_extent(input_b.buffer, 1)
let input_b.stride.1 = _halide_buffer_get_stride(input_b.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.type = _halide_buffer_get_type(output.buffer)
let output.dimensions = _halide_buffer_get_dimensions(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
if (_halide_buffer_is_bounds_query(input_a.buffer)) {
  _halide_buffer_init(input_a.buffer, _halide_buffer_get_shape(input_a.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (_halide_buffer_is_bounds_query(input_b.buffer)) {
  _halide_buffer_init(input_b.buffer, _halide_buffer_get_shape(input_b.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (_halide_buffer_is_bounds_query(output.buffer)) {
  _halide_buffer_init(output.buffer, _halide_buffer_get_shape(output.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (!(_halide_buffer_is_bounds_query(output.buffer) || (_halide_buffer_is_bounds_query(input_a.buffer) || _halide_buffer_is_bounds_query(input_b.buffer)))) {
  assert((input_a.type == (uint32)73730), halide_error_bad_type("Input buffer input_a", input_a.type, (uint32)73730))
  assert((input_a.dimensions == 2), halide_error_bad_dimensions("Input buffer input_a", input_a.dimensions, 2))
  assert((input_b.type == (uint32)73730), halide_error_bad_type("Input buffer input_b", input_b.type, (uint32)73730))
  assert((input_b.dimensions == 2), halide_error_bad_dimensions("Input buffer input_b", input_b.dimensions, 2))
  assert((output.type == (uint32)73730), halide_error_bad_type("Output buffer output", output.type, (uint32)73730))
  assert((output.dimensions == 2), halide_error_bad_dimensions("Output buffer output", output.dimensions, 2))
  assert(((input_a.min.0 <= 0) && (1536 <= (input_a.extent.0 + input_a.min.0))), halide_error_access_out_of_bounds("Input buffer input_a", 0, 0, 1535, input_a.min.0, ((input_a.extent.0 + input_a.min.0) + -1)))
  assert((0 <= input_a.extent.0), halide_error_buffer_extents_negative("Input buffer input_a", 0, input_a.extent.0))
  assert(((input_a.min.1 <= 0) && (1536 <= (input_a.extent.1 + input_a.min.1))), halide_error_access_out_of_bounds("Input buffer input_a", 1, 0, 1535, input_a.min.1, ((input_a.extent.1 + input_a.min.1) + -1)))
  assert((0 <= input_a.extent.1), halide_error_buffer_extents_negative("Input buffer input_a", 1, input_a.extent.1))
  assert(((input_b.min.0 <= 0) && (1536 <= (input_b.extent.0 + input_b.min.0))), halide_error_access_out_of_bounds("Input buffer input_b", 0, 0, 1535, input_b.min.0, ((input_b.extent.0 + input_b.min.0) + -1)))
  assert((0 <= input_b.extent.0), halide_error_buffer_extents_negative("Input buffer input_b", 0, input_b.extent.0))
  assert(((input_b.min.1 <= 0) && (1536 <= (input_b.extent.1 + input_b.min.1))), halide_error_access_out_of_bounds("Input buffer input_b", 1, 0, 1535, input_b.min.1, ((input_b.extent.1 + input_b.min.1) + -1)))
  assert((0 <= input_b.extent.1), halide_error_buffer_extents_negative("Input buffer input_b", 1, input_b.extent.1))
  assert(((output.min.0 <= 0) && (1536 <= (output.extent.0 + output.min.0))), halide_error_access_out_of_bounds("Output buffer output", 0, 0, 1535, output.min.0, ((output.extent.0 + output.min.0) + -1)))
  assert((0 <= output.extent.0), halide_error_buffer_extents_negative("Output buffer output", 0, output.extent.0))
  assert(((output.min.1 <= 0) && (1536 <= (output.extent.1 + output.min.1))), halide_error_access_out_of_bounds("Output buffer output", 1, 0, 1535, output.min.1, ((output.extent.1 + output.min.1) + -1)))
  assert((0 <= output.extent.1), halide_error_buffer_extents_negative("Output buffer output", 1, output.extent.1))
  assert((input_a.stride.0 == 1), halide_error_constraint_violated("input_a.stride.0", input_a.stride.0, "1", 1))
  assert((input_b.stride.0 == 1), halide_error_constraint_violated("input_b.stride.0", input_b.stride.0, "1", 1))
  assert((output.stride.0 == 1), halide_error_constraint_violated("output.stride.0", output.stride.0, "1", 1))
  let input_a.total_extent.1 = (int64(input_a.extent.1)*int64(input_a.extent.0))
  let input_b.total_extent.1 = (int64(input_b.extent.1)*int64(input_b.extent.0))
  let output.total_extent.1 = (int64(output.extent.1)*int64(output.extent.0))
  assert((abs(int64(input_a.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_a", abs(int64(input_a.extent.0)), (uint64)2147483647))
  assert((abs((int64(input_a.extent.1)*int64(input_a.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_a", abs((int64(input_a.extent.1)*int64(input_a.stride.1))), (uint64)2147483647))
  assert((input_a.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("input_a", input_a.total_extent.1, (int64)2147483647))
  assert((abs(int64(input_b.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_b", abs(int64(input_b.extent.0)), (uint64)2147483647))
  assert((abs((int64(input_b.extent.1)*int64(input_b.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_b", abs((int64(input_b.extent.1)*int64(input_b.stride.1))), (uint64)2147483647))
  assert((input_b.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("input_b", input_b.total_extent.1, (int64)2147483647))
  assert((abs(int64(output.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("output", abs(int64(output.extent.0)), (uint64)2147483647))
  assert((abs((int64(output.extent.1)*int64(output.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("output", abs((int64(output.extent.1)*int64(output.stride.1))), (uint64)2147483647))
  assert((output.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("output", output.total_extent.1, (int64)2147483647))
  allocate matrix_mul[float32 * 1536 * 1536] if (uint1)0
  let matrix_mul.buffer = (let t9 = make_struct((halide_dimension_t *), 0, 1536, 1536, 0, 0, 1536, 1, 0) in _halide_buffer_init(alloca(size_of_halide_buffer_t()), t9, reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, t9, (uint64)0))
  register_destructor("halide_device_free_as_destructor", matrix_mul.buffer)
  produce matrix_mul {
    let halide_device_malloc_result = halide_device_malloc(matrix_mul.buffer, halide_cuda_device_interface())
    assert((halide_device_malloc_result == 0), halide_device_malloc_result)
    gpu_block<CUDA> (matrix_mul.s0.x.x.__block_id_y, 0, 3) {
      gpu_block<CUDA> (matrix_mul.s0.y.y.__block_id_x, 0, 24) {
        gpu_thread<CUDA> (matrix_mul.s0.x.xi.xi.xi.__thread_id_y, 0, 64) {
          gpu_thread<CUDA> (matrix_mul.s0.y.yi.yi.yi.__thread_id_x, 0, 16) {
            for (matrix_mul.s0.x.xi.xii, 0, 8) {
              for (matrix_mul.s0.y.yi.yii, 0, 4) {
                matrix_mul[(((min(likely_if_innermost((matrix_mul.s0.x.x.__block_id_y*512)), 1024) + ((matrix_mul.s0.x.xi.xi.xi.__thread_id_y*8) + matrix_mul.s0.x.xi.xii))*1536) + (min(likely_if_innermost((matrix_mul.s0.y.y.__block_id_x*64)), 1472) + ((matrix_mul.s0.y.yi.yi.yi.__thread_id_x*4) + matrix_mul.s0.y.yi.yii)))] = 0.000000f
              }
            }
          }
        }
      }
    }
    _halide_buffer_set_device_dirty(matrix_mul.buffer, (uint1)1)
    let halide_copy_to_device_result = halide_copy_to_device(input_a.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result == 0), halide_copy_to_device_result)
    let halide_copy_to_device_result$1 = halide_copy_to_device(input_b.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result$1 == 0), halide_copy_to_device_result$1)
    gpu_block<CUDA> (matrix_mul.s1.x.x.__block_id_y, 0, 3) {
      gpu_block<CUDA> (matrix_mul.s1.y.y.__block_id_x, 0, 24) {
        gpu_thread<CUDA> (matrix_mul.s1.x.xi.xi.xi.__thread_id_y, 0, 64) {
          gpu_thread<CUDA> (matrix_mul.s1.y.yi.yi.yi.__thread_id_x, 0, 16) {
            for (matrix_mul.s1.x.xi.xii, 0, 8) {
              let matrix_mul.s1.x.rebased = ((matrix_mul.s1.x.x.__block_id_y*512) + ((matrix_mul.s1.x.xi.xi.xi.__thread_id_y*8) + matrix_mul.s1.x.xi.xii))
              if (likely((uint1)1)) {
                for (matrix_mul.s1.y.yi.yii, 0, 4) {
                  let matrix_mul.s1.y.rebased = ((matrix_mul.s1.y.y.__block_id_x*64) + ((matrix_mul.s1.y.yi.yi.yi.__thread_id_x*4) + matrix_mul.s1.y.yi.yii))
                  for (matrix_mul.s1.r4$x, 0, 1536) {
                    matrix_mul[((matrix_mul.s1.x.rebased*1536) + matrix_mul.s1.y.rebased)] = (matrix_mul[((matrix_mul.s1.x.rebased*1536) + matrix_mul.s1.y.rebased)] + (input_a[(((input_a.stride.1*matrix_mul.s1.y.rebased) + matrix_mul.s1.r4$x) - ((input_a.min.1*input_a.stride.1) + input_a.min.0))]*input_b[(((input_b.stride.1*matrix_mul.s1.r4$x) + matrix_mul.s1.x.rebased) - ((input_b.min.1*input_b.stride.1) + input_b.min.0))]))
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  assert(((0 <= output.min.1) && ((output.extent.1 + output.min.1) <= 1536)), halide_error_explicit_bounds_too_small("y", "output", 0, 1535, output.min.1, ((output.extent.1 + output.min.1) + -1)))
  assert(((0 <= output.min.0) && ((output.extent.0 + output.min.0) <= 1536)), halide_error_explicit_bounds_too_small("x", "output", 0, 1535, output.min.0, ((output.extent.0 + output.min.0) + -1)))
  produce output {
    consume matrix_mul {
      let halide_copy_to_device_result$2 = halide_copy_to_device(output.buffer, halide_cuda_device_interface())
      assert((halide_copy_to_device_result$2 == 0), halide_copy_to_device_result$2)
      gpu_block<CUDA> (output.s0.y.y.__block_id_y, 0, 12) {
        gpu_block<CUDA> (output.s0.x.x.__block_id_x, 0, 96) {
          gpu_thread<CUDA> (output.s0.y.yi.yi.yi.__thread_id_y, 0, 64) {
            gpu_thread<CUDA> (output.s0.x.xi.xi.__thread_id_x, 0, 16) {
              unrolled (output.s0.y.yi.yii, 0, 2) {
                output[((((output.s0.y.y.__block_id_y*128) + ((output.s0.y.yi.yi.yi.__thread_id_y*2) + output.s0.y.yi.yii))*output.stride.1) + ((output.s0.x.x.__block_id_x*16) + output.s0.x.xi.xi.__thread_id_x))] = matrix_mul[((((output.s0.x.x.__block_id_x*16) + output.s0.x.xi.xi.__thread_id_x)*1536) + ((output.s0.y.y.__block_id_y*128) + ((output.s0.y.yi.yi.yi.__thread_id_y*2) + output.s0.y.yi.yii)))]
              }
            }
          }
        }
      }
      _halide_buffer_set_device_dirty(output.buffer, (uint1)1)
      let halide_device_free_result = halide_device_free(matrix_mul.buffer)
      assert((halide_device_free_result == 0), halide_device_free_result)
    }
  }
}

Simplifying correlated differences...
Unrolling...
Vectorizing...
Injecting per-block gpu synchronization...
Detecting vector interleavings...
Partitioning loops to simplify boundary conditions...
Trimming loops to the region over which they do something...
Injecting early frees...
Simplifying correlated differences...
Bounding small allocations...
Injecting warp shuffles...
Simplifying...
Lowering unsafe promises...
Lowering after final simplification:
assert((reinterpret(uint64, output.buffer) != (uint64)0), halide_error_buffer_argument_is_null("output"))
assert((reinterpret(uint64, input_b.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input_b"))
assert((reinterpret(uint64, input_a.buffer) != (uint64)0), halide_error_buffer_argument_is_null("input_a"))
let input_a = _halide_buffer_get_host(input_a.buffer)
let input_a.type = _halide_buffer_get_type(input_a.buffer)
let input_a.dimensions = _halide_buffer_get_dimensions(input_a.buffer)
let input_a.min.0 = _halide_buffer_get_min(input_a.buffer, 0)
let input_a.extent.0 = _halide_buffer_get_extent(input_a.buffer, 0)
let input_a.stride.0 = _halide_buffer_get_stride(input_a.buffer, 0)
let input_a.min.1 = _halide_buffer_get_min(input_a.buffer, 1)
let input_a.extent.1 = _halide_buffer_get_extent(input_a.buffer, 1)
let input_a.stride.1 = _halide_buffer_get_stride(input_a.buffer, 1)
let input_b = _halide_buffer_get_host(input_b.buffer)
let input_b.type = _halide_buffer_get_type(input_b.buffer)
let input_b.dimensions = _halide_buffer_get_dimensions(input_b.buffer)
let input_b.min.0 = _halide_buffer_get_min(input_b.buffer, 0)
let input_b.extent.0 = _halide_buffer_get_extent(input_b.buffer, 0)
let input_b.stride.0 = _halide_buffer_get_stride(input_b.buffer, 0)
let input_b.min.1 = _halide_buffer_get_min(input_b.buffer, 1)
let input_b.extent.1 = _halide_buffer_get_extent(input_b.buffer, 1)
let input_b.stride.1 = _halide_buffer_get_stride(input_b.buffer, 1)
let output = _halide_buffer_get_host(output.buffer)
let output.type = _halide_buffer_get_type(output.buffer)
let output.dimensions = _halide_buffer_get_dimensions(output.buffer)
let output.min.0 = _halide_buffer_get_min(output.buffer, 0)
let output.extent.0 = _halide_buffer_get_extent(output.buffer, 0)
let output.stride.0 = _halide_buffer_get_stride(output.buffer, 0)
let output.min.1 = _halide_buffer_get_min(output.buffer, 1)
let output.extent.1 = _halide_buffer_get_extent(output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride(output.buffer, 1)
if (_halide_buffer_is_bounds_query(input_a.buffer)) {
  _halide_buffer_init(input_a.buffer, _halide_buffer_get_shape(input_a.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (_halide_buffer_is_bounds_query(input_b.buffer)) {
  _halide_buffer_init(input_b.buffer, _halide_buffer_get_shape(input_b.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (_halide_buffer_is_bounds_query(output.buffer)) {
  _halide_buffer_init(output.buffer, _halide_buffer_get_shape(output.buffer), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1, 0, 0, 1536, 1536, 0), (uint64)0)
}
if (!(_halide_buffer_is_bounds_query(output.buffer) || (_halide_buffer_is_bounds_query(input_a.buffer) || _halide_buffer_is_bounds_query(input_b.buffer)))) {
  assert((input_a.type == (uint32)73730), halide_error_bad_type("Input buffer input_a", input_a.type, (uint32)73730))
  assert((input_a.dimensions == 2), halide_error_bad_dimensions("Input buffer input_a", input_a.dimensions, 2))
  assert((input_b.type == (uint32)73730), halide_error_bad_type("Input buffer input_b", input_b.type, (uint32)73730))
  assert((input_b.dimensions == 2), halide_error_bad_dimensions("Input buffer input_b", input_b.dimensions, 2))
  assert((output.type == (uint32)73730), halide_error_bad_type("Output buffer output", output.type, (uint32)73730))
  assert((output.dimensions == 2), halide_error_bad_dimensions("Output buffer output", output.dimensions, 2))
  assert(((input_a.min.0 <= 0) && (1536 <= (input_a.extent.0 + input_a.min.0))), halide_error_access_out_of_bounds("Input buffer input_a", 0, 0, 1535, input_a.min.0, ((input_a.extent.0 + input_a.min.0) + -1)))
  assert((0 <= input_a.extent.0), halide_error_buffer_extents_negative("Input buffer input_a", 0, input_a.extent.0))
  assert(((input_a.min.1 <= 0) && (1536 <= (input_a.extent.1 + input_a.min.1))), halide_error_access_out_of_bounds("Input buffer input_a", 1, 0, 1535, input_a.min.1, ((input_a.extent.1 + input_a.min.1) + -1)))
  assert((0 <= input_a.extent.1), halide_error_buffer_extents_negative("Input buffer input_a", 1, input_a.extent.1))
  assert(((input_b.min.0 <= 0) && (1536 <= (input_b.extent.0 + input_b.min.0))), halide_error_access_out_of_bounds("Input buffer input_b", 0, 0, 1535, input_b.min.0, ((input_b.extent.0 + input_b.min.0) + -1)))
  assert((0 <= input_b.extent.0), halide_error_buffer_extents_negative("Input buffer input_b", 0, input_b.extent.0))
  assert(((input_b.min.1 <= 0) && (1536 <= (input_b.extent.1 + input_b.min.1))), halide_error_access_out_of_bounds("Input buffer input_b", 1, 0, 1535, input_b.min.1, ((input_b.extent.1 + input_b.min.1) + -1)))
  assert((0 <= input_b.extent.1), halide_error_buffer_extents_negative("Input buffer input_b", 1, input_b.extent.1))
  assert(((output.min.0 <= 0) && (1536 <= (output.extent.0 + output.min.0))), halide_error_access_out_of_bounds("Output buffer output", 0, 0, 1535, output.min.0, ((output.extent.0 + output.min.0) + -1)))
  assert((0 <= output.extent.0), halide_error_buffer_extents_negative("Output buffer output", 0, output.extent.0))
  assert(((output.min.1 <= 0) && (1536 <= (output.extent.1 + output.min.1))), halide_error_access_out_of_bounds("Output buffer output", 1, 0, 1535, output.min.1, ((output.extent.1 + output.min.1) + -1)))
  assert((0 <= output.extent.1), halide_error_buffer_extents_negative("Output buffer output", 1, output.extent.1))
  assert((input_a.stride.0 == 1), halide_error_constraint_violated("input_a.stride.0", input_a.stride.0, "1", 1))
  assert((input_b.stride.0 == 1), halide_error_constraint_violated("input_b.stride.0", input_b.stride.0, "1", 1))
  assert((output.stride.0 == 1), halide_error_constraint_violated("output.stride.0", output.stride.0, "1", 1))
  let input_a.total_extent.1 = (int64(input_a.extent.1)*int64(input_a.extent.0))
  let input_b.total_extent.1 = (int64(input_b.extent.1)*int64(input_b.extent.0))
  let output.total_extent.1 = (int64(output.extent.1)*int64(output.extent.0))
  assert((abs(int64(input_a.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_a", abs(int64(input_a.extent.0)), (uint64)2147483647))
  assert((abs((int64(input_a.extent.1)*int64(input_a.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_a", abs((int64(input_a.extent.1)*int64(input_a.stride.1))), (uint64)2147483647))
  assert((input_a.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("input_a", input_a.total_extent.1, (int64)2147483647))
  assert((abs(int64(input_b.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_b", abs(int64(input_b.extent.0)), (uint64)2147483647))
  assert((abs((int64(input_b.extent.1)*int64(input_b.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("input_b", abs((int64(input_b.extent.1)*int64(input_b.stride.1))), (uint64)2147483647))
  assert((input_b.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("input_b", input_b.total_extent.1, (int64)2147483647))
  assert((abs(int64(output.extent.0)) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("output", abs(int64(output.extent.0)), (uint64)2147483647))
  assert((abs((int64(output.extent.1)*int64(output.stride.1))) <= (uint64)2147483647), halide_error_buffer_allocation_too_large("output", abs((int64(output.extent.1)*int64(output.stride.1))), (uint64)2147483647))
  assert((output.total_extent.1 <= (int64)2147483647), halide_error_buffer_extents_too_large("output", output.total_extent.1, (int64)2147483647))
  allocate matrix_mul[float32 * 1536 * 1536] if (uint1)0
  let matrix_mul.buffer = _halide_buffer_init(alloca(size_of_halide_buffer_t()), make_struct((halide_dimension_t *), 0, 1536, 1536, 0, 0, 1536, 1, 0), reinterpret((void *), (uint64)0), (uint64)0, reinterpret((halide_device_interface_t *), (uint64)0), 2, 32, 2, make_struct((halide_dimension_t *), 0, 1536, 1536, 0, 0, 1536, 1, 0), (uint64)0)
  register_destructor("halide_device_free_as_destructor", matrix_mul.buffer)
  produce matrix_mul {
    let halide_device_malloc_result = halide_device_malloc(matrix_mul.buffer, halide_cuda_device_interface())
    assert((halide_device_malloc_result == 0), halide_device_malloc_result)
    gpu_block<CUDA> (matrix_mul.s0.x.x.__block_id_y, 0, 3) {
      gpu_block<CUDA> (matrix_mul.s0.y.y.__block_id_x, 0, 24) {
        gpu_thread<CUDA> (.__thread_id_y, 0, 64) {
          gpu_thread<CUDA> (.__thread_id_x, 0, 16) {
            let t29 = (((matrix_mul.s0.x.x.__block_id_y*64) + .__thread_id_y)*8)
            let t28 = ((matrix_mul.s0.y.y.__block_id_x*16) + .__thread_id_x)
            for (matrix_mul.s0.x.xi.xii, 0, 8) {
              let t19.s = (((matrix_mul.s0.x.xi.xii + t29)*384) + t28)
              let t30 = (t19.s*4)
              for (matrix_mul.s0.y.yi.yii, 0, 4) {
                matrix_mul[(matrix_mul.s0.y.yi.yii + t30)] = 0.000000f
              }
            }
          }
        }
      }
    }
    _halide_buffer_set_device_dirty(matrix_mul.buffer, (uint1)1)
    let halide_copy_to_device_result = halide_copy_to_device(input_a.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result == 0), halide_copy_to_device_result)
    let halide_copy_to_device_result$1 = halide_copy_to_device(input_b.buffer, halide_cuda_device_interface())
    assert((halide_copy_to_device_result$1 == 0), halide_copy_to_device_result$1)
    let t31 = ((input_b.min.1*input_b.stride.1) + input_b.min.0)
    let t32 = ((input_a.min.1*input_a.stride.1) + input_a.min.0)
    gpu_block<CUDA> (matrix_mul.s1.x.x.__block_id_y, 0, 3) {
      gpu_block<CUDA> (matrix_mul.s1.y.y.__block_id_x, 0, 24) {
        gpu_thread<CUDA> (.__thread_id_y, 0, 64) {
          gpu_thread<CUDA> (.__thread_id_x, 0, 16) {
            let t36 = (((matrix_mul.s1.x.x.__block_id_y*64) + .__thread_id_y)*8)
            let t34 = (((matrix_mul.s1.y.y.__block_id_x*16) + .__thread_id_x)*4)
            let t33 = (t36 - t31)
            for (matrix_mul.s1.x.xi.xii, 0, 8) {
              let t25 = (matrix_mul.s1.x.xi.xii + t33)
              let t37 = (((matrix_mul.s1.x.xi.xii + t36)*1536) + t34)
              for (matrix_mul.s1.y.yi.yii, 0, 4) {
                let t27 = (((matrix_mul.s1.y.yi.yii + t34)*input_a.stride.1) - t32)
                let t26 = (matrix_mul.s1.y.yi.yii + t37)
                for (matrix_mul.s1.r4$x, 0, 1536) {
                  matrix_mul[t26] = (matrix_mul[t26] + (input_a[(matrix_mul.s1.r4$x + t27)]*input_b[((input_b.stride.1*matrix_mul.s1.r4$x) + t25)]))
                }
              }
            }
          }
        }
      }
    }
  }
  assert(((0 <= output.min.1) && ((output.extent.1 + output.min.1) <= 1536)), halide_error_explicit_bounds_too_small("y", "output", 0, 1535, output.min.1, ((output.extent.1 + output.min.1) + -1)))
  assert(((0 <= output.min.0) && ((output.extent.0 + output.min.0) <= 1536)), halide_error_explicit_bounds_too_small("x", "output", 0, 1535, output.min.0, ((output.extent.0 + output.min.0) + -1)))
  produce output {
    consume matrix_mul {
      let halide_copy_to_device_result$2 = halide_copy_to_device(output.buffer, halide_cuda_device_interface())
      assert((halide_copy_to_device_result$2 == 0), halide_copy_to_device_result$2)
      gpu_block<CUDA> (output.s0.y.y.__block_id_y, 0, 12) {
        gpu_block<CUDA> (output.s0.x.x.__block_id_x, 0, 96) {
          gpu_thread<CUDA> (.__thread_id_y, 0, 64) {
            gpu_thread<CUDA> (.__thread_id_x, 0, 16) {
              output[((((output.s0.x.x.__block_id_x*8) + (((output.s0.y.y.__block_id_y*64) + .__thread_id_y)*output.stride.1))*2) + .__thread_id_x)] = matrix_mul[(((((output.s0.x.x.__block_id_x*16) + .__thread_id_x)*768) + ((output.s0.y.y.__block_id_y*64) + .__thread_id_y))*2)]
              output[(((output.s0.x.x.__block_id_x*16) + (((((output.s0.y.y.__block_id_y*64) + .__thread_id_y)*2) + 1)*output.stride.1)) + .__thread_id_x)] = matrix_mul[((((((output.s0.x.x.__block_id_x*16) + .__thread_id_x)*768) + ((output.s0.y.y.__block_id_y*64) + .__thread_id_y))*2) + 1)]
            }
          }
        }
      }
      _halide_buffer_set_device_dirty(output.buffer, (uint1)1)
      let halide_device_free_result = halide_device_free(matrix_mul.buffer)
      assert((halide_device_free_result == 0), halide_device_free_result)
      free matrix_mul
    }
  }
}


Skipping Hexagon offload...
Module.compile(): stmt_name ./bin/mat_mul_classic_auto_schedule.stmt
Constructing CUDA device codegen
Target triple of initial module: x86_64--linux-gnu
Generating llvm bitcode...
Generating llvm bitcode prolog for function mat_mul_classic_auto_schedule...
Generating llvm bitcode for function mat_mul_classic_auto_schedule...
Generating llvm bitcode for kernel...
Generating llvm bitcode for kernel...
Generating llvm bitcode for kernel...
PTX kernel:
//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_35
.address_size 64

	// .globl	kernel_matrix_mul_s0_x_x___block_id_y // -- Begin function kernel_matrix_mul_s0_x_x___block_id_y
                                        // @kernel_matrix_mul_s0_x_x___block_id_y
.visible .entry kernel_matrix_mul_s0_x_x___block_id_y(
	.param .u64 kernel_matrix_mul_s0_x_x___block_id_y_param_0
)
{
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<19>;

// %bb.0:                               // %entry
	ld.param.u64 	%rd1, [kernel_matrix_mul_s0_x_x___block_id_y_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.y;
	mov.u32 	%r4, %tid.x;
	shl.b32 	%r5, %r2, 4;
	mul.lo.s32 	%r6, %r1, 196608;
	or.b32  	%r7, %r4, %r6;
	mad.lo.s32 	%r8, %r3, 3072, %r7;
	add.s32 	%r9, %r8, %r5;
	shl.b32 	%r10, %r9, 2;
	mul.wide.s32 	%rd3, %r10, 4;
	add.s64 	%rd4, %rd2, %rd3;
	mov.u32 	%r11, 0;
	st.global.u32 	[%rd4+12], %r11;
	st.global.u32 	[%rd4+8], %r11;
	st.global.u32 	[%rd4+4], %r11;
	st.global.u32 	[%rd4], %r11;
	add.s32 	%r12, %r10, 1536;
	mul.wide.s32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.u32 	[%rd6+12], %r11;
	st.global.u32 	[%rd6+8], %r11;
	st.global.u32 	[%rd6+4], %r11;
	st.global.u32 	[%rd6], %r11;
	add.s32 	%r13, %r10, 3072;
	mul.wide.s32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.u32 	[%rd8+12], %r11;
	st.global.u32 	[%rd8+8], %r11;
	st.global.u32 	[%rd8+4], %r11;
	st.global.u32 	[%rd8], %r11;
	add.s32 	%r14, %r10, 4608;
	mul.wide.s32 	%rd9, %r14, 4;
	add.s64 	%rd10, %rd2, %rd9;
	st.global.u32 	[%rd10+12], %r11;
	st.global.u32 	[%rd10+8], %r11;
	st.global.u32 	[%rd10+4], %r11;
	st.global.u32 	[%rd10], %r11;
	add.s32 	%r15, %r10, 6144;
	mul.wide.s32 	%rd11, %r15, 4;
	add.s64 	%rd12, %rd2, %rd11;
	st.global.u32 	[%rd12+12], %r11;
	st.global.u32 	[%rd12+8], %r11;
	st.global.u32 	[%rd12+4], %r11;
	st.global.u32 	[%rd12], %r11;
	add.s32 	%r16, %r10, 7680;
	mul.wide.s32 	%rd13, %r16, 4;
	add.s64 	%rd14, %rd2, %rd13;
	st.global.u32 	[%rd14+12], %r11;
	st.global.u32 	[%rd14+8], %r11;
	st.global.u32 	[%rd14+4], %r11;
	st.global.u32 	[%rd14], %r11;
	add.s32 	%r17, %r10, 9216;
	mul.wide.s32 	%rd15, %r17, 4;
	add.s64 	%rd16, %rd2, %rd15;
	st.global.u32 	[%rd16+12], %r11;
	st.global.u32 	[%rd16+8], %r11;
	st.global.u32 	[%rd16+4], %r11;
	st.global.u32 	[%rd16], %r11;
	add.s32 	%r18, %r10, 10752;
	mul.wide.s32 	%rd17, %r18, 4;
	add.s64 	%rd18, %rd2, %rd17;
	st.global.u32 	[%rd18+12], %r11;
	st.global.u32 	[%rd18+8], %r11;
	st.global.u32 	[%rd18+4], %r11;
	st.global.u32 	[%rd18], %r11;
	ret;
                                        // -- End function
}
	// .globl	kernel_matrix_mul_s1_x_x___block_id_y // -- Begin function kernel_matrix_mul_s1_x_x___block_id_y
.visible .entry kernel_matrix_mul_s1_x_x___block_id_y(
	.param .u32 kernel_matrix_mul_s1_x_x___block_id_y_param_0,
	.param .u32 kernel_matrix_mul_s1_x_x___block_id_y_param_1,
	.param .u32 kernel_matrix_mul_s1_x_x___block_id_y_param_2,
	.param .u32 kernel_matrix_mul_s1_x_x___block_id_y_param_3,
	.param .u64 kernel_matrix_mul_s1_x_x___block_id_y_param_4,
	.param .u64 kernel_matrix_mul_s1_x_x___block_id_y_param_5,
	.param .u64 kernel_matrix_mul_s1_x_x___block_id_y_param_6
)                                       // @kernel_matrix_mul_s1_x_x___block_id_y
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<85>;
	.reg .b32 	%r<170>;
	.reg .b64 	%rd<78>;

// %bb.0:                               // %entry
	ld.param.u32 	%r82, [kernel_matrix_mul_s1_x_x___block_id_y_param_0];
	ld.param.u64 	%rd18, [kernel_matrix_mul_s1_x_x___block_id_y_param_6];
	cvta.to.global.u64 	%rd1, %rd18;
	ld.param.u32 	%r83, [kernel_matrix_mul_s1_x_x___block_id_y_param_1];
	ld.param.u64 	%rd19, [kernel_matrix_mul_s1_x_x___block_id_y_param_5];
	cvta.to.global.u64 	%rd2, %rd19;
	ld.param.u32 	%r84, [kernel_matrix_mul_s1_x_x___block_id_y_param_2];
	ld.param.u64 	%rd20, [kernel_matrix_mul_s1_x_x___block_id_y_param_4];
	cvta.to.global.u64 	%rd3, %rd20;
	ld.param.u32 	%r85, [kernel_matrix_mul_s1_x_x___block_id_y_param_3];
	mov.u32 	%r86, %ctaid.y;
	mov.u32 	%r87, %ctaid.x;
	mov.u32 	%r88, %tid.y;
	mov.u32 	%r89, %tid.x;
	shl.b32 	%r90, %r86, 6;
	add.s32 	%r91, %r88, %r90;
	shl.b32 	%r1, %r91, 3;
	shl.b32 	%r92, %r87, 4;
	add.s32 	%r93, %r89, %r92;
	shl.b32 	%r2, %r93, 2;
	sub.s32 	%r139, %r1, %r84;
	mul.lo.s32 	%r94, %r2, %r82;
	sub.s32 	%r4, %r94, %r85;
	or.b32  	%r5, %r2, 1;
	add.s32 	%r95, %r94, %r82;
	sub.s32 	%r6, %r95, %r85;
	or.b32  	%r7, %r2, 2;
	add.s32 	%r96, %r95, %r82;
	sub.s32 	%r8, %r96, %r85;
	or.b32  	%r9, %r2, 3;
	add.s32 	%r97, %r96, %r82;
	sub.s32 	%r10, %r97, %r85;
	shl.b32 	%r11, %r88, 3;
	shl.b32 	%r98, %r86, 9;
	add.s32 	%r99, %r83, %r98;
	sub.s32 	%r144, %r99, %r84;
	mul.lo.s32 	%r13, %r83, 6;
	shl.b32 	%r100, %r83, 1;
	add.s32 	%r101, %r98, %r100;
	sub.s32 	%r143, %r101, %r84;
	mad.lo.s32 	%r102, %r83, 3, %r98;
	sub.s32 	%r142, %r102, %r84;
	shl.b32 	%r103, %r83, 2;
	add.s32 	%r104, %r98, %r103;
	sub.s32 	%r141, %r104, %r84;
	mad.lo.s32 	%r105, %r83, 5, %r98;
	sub.s32 	%r140, %r105, %r84;
	mul.wide.s32 	%rd4, %r13, 4;
	mov.u32 	%r81, 0;
	mov.u32 	%r145, %r81;
LBB1_1:                                 // %"for matrix_mul.s1.x.xi.xii"
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB1_2 Depth 2
                                        //     Child Loop BB1_4 Depth 2
                                        //     Child Loop BB1_6 Depth 2
                                        //     Child Loop BB1_8 Depth 2
	mul.wide.s32 	%rd21, %r139, 4;
	add.s64 	%rd77, %rd2, %rd21;
	add.s32 	%r107, %r145, %r1;
	mul.lo.s32 	%r25, %r107, 1536;
	add.s32 	%r108, %r2, %r25;
	mul.wide.s32 	%rd22, %r108, 4;
	add.s64 	%rd6, %rd1, %rd22;
	ld.global.f32 	%f81, [%rd6];
	mov.u64 	%rd74, %rd77;
	mov.u32 	%r146, %r140;
	mov.u32 	%r147, %r141;
	mov.u32 	%r148, %r142;
	mov.u32 	%r149, %r143;
	mov.u32 	%r150, %r144;
	mov.u32 	%r151, %r81;
LBB1_2:                                 // %"for matrix_mul.s1.r4$x"
                                        //   Parent Loop BB1_1 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	add.s32 	%r109, %r4, %r151;
	mul.wide.s32 	%rd23, %r109, 4;
	add.s64 	%rd24, %rd3, %rd23;
	ld.global.nc.f32 	%f13, [%rd24];
	ld.global.nc.f32 	%f14, [%rd74];
	fma.rn.ftz.f32 	%f15, %f13, %f14, %f81;
	ld.global.nc.f32 	%f16, [%rd24+4];
	add.s32 	%r110, %r11, %r150;
	mul.wide.s32 	%rd25, %r110, 4;
	add.s64 	%rd26, %rd2, %rd25;
	ld.global.nc.f32 	%f17, [%rd26];
	fma.rn.ftz.f32 	%f18, %f16, %f17, %f15;
	ld.global.nc.f32 	%f19, [%rd24+8];
	add.s32 	%r111, %r11, %r149;
	mul.wide.s32 	%rd27, %r111, 4;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.f32 	%f20, [%rd28];
	fma.rn.ftz.f32 	%f21, %f19, %f20, %f18;
	ld.global.nc.f32 	%f22, [%rd24+12];
	add.s32 	%r112, %r11, %r148;
	mul.wide.s32 	%rd29, %r112, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f23, [%rd30];
	fma.rn.ftz.f32 	%f24, %f22, %f23, %f21;
	ld.global.nc.f32 	%f25, [%rd24+16];
	add.s32 	%r113, %r11, %r147;
	mul.wide.s32 	%rd31, %r113, 4;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.nc.f32 	%f26, [%rd32];
	fma.rn.ftz.f32 	%f27, %f25, %f26, %f24;
	ld.global.nc.f32 	%f28, [%rd24+20];
	add.s32 	%r114, %r11, %r146;
	mul.wide.s32 	%rd33, %r114, 4;
	add.s64 	%rd34, %rd2, %rd33;
	ld.global.nc.f32 	%f29, [%rd34];
	fma.rn.ftz.f32 	%f81, %f28, %f29, %f27;
	add.s32 	%r151, %r151, 6;
	add.s32 	%r150, %r150, %r13;
	add.s32 	%r149, %r149, %r13;
	add.s32 	%r148, %r148, %r13;
	add.s32 	%r147, %r147, %r13;
	add.s32 	%r146, %r146, %r13;
	add.s64 	%rd74, %rd74, %rd4;
	setp.ne.s32 	%p1, %r151, 1536;
	@%p1 bra 	LBB1_2;
// %bb.3:                               // %"end for matrix_mul.s1.r4$x"
                                        //   in Loop: Header=BB1_1 Depth=1
	st.global.f32 	[%rd6], %f81;
	add.s32 	%r116, %r5, %r25;
	mul.wide.s32 	%rd35, %r116, 4;
	add.s64 	%rd9, %rd1, %rd35;
	ld.global.f32 	%f82, [%rd9];
	mov.u32 	%r157, 0;
	mov.u64 	%rd75, %rd77;
	mov.u32 	%r152, %r140;
	mov.u32 	%r153, %r141;
	mov.u32 	%r154, %r142;
	mov.u32 	%r155, %r143;
	mov.u32 	%r156, %r144;
LBB1_4:                                 // %"for matrix_mul.s1.r4$x.1"
                                        //   Parent Loop BB1_1 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	add.s32 	%r117, %r6, %r157;
	mul.wide.s32 	%rd36, %r117, 4;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.nc.f32 	%f30, [%rd37];
	ld.global.nc.f32 	%f31, [%rd75];
	fma.rn.ftz.f32 	%f32, %f30, %f31, %f82;
	ld.global.nc.f32 	%f33, [%rd37+4];
	add.s32 	%r118, %r11, %r156;
	mul.wide.s32 	%rd38, %r118, 4;
	add.s64 	%rd39, %rd2, %rd38;
	ld.global.nc.f32 	%f34, [%rd39];
	fma.rn.ftz.f32 	%f35, %f33, %f34, %f32;
	ld.global.nc.f32 	%f36, [%rd37+8];
	add.s32 	%r119, %r11, %r155;
	mul.wide.s32 	%rd40, %r119, 4;
	add.s64 	%rd41, %rd2, %rd40;
	ld.global.nc.f32 	%f37, [%rd41];
	fma.rn.ftz.f32 	%f38, %f36, %f37, %f35;
	ld.global.nc.f32 	%f39, [%rd37+12];
	add.s32 	%r120, %r11, %r154;
	mul.wide.s32 	%rd42, %r120, 4;
	add.s64 	%rd43, %rd2, %rd42;
	ld.global.nc.f32 	%f40, [%rd43];
	fma.rn.ftz.f32 	%f41, %f39, %f40, %f38;
	ld.global.nc.f32 	%f42, [%rd37+16];
	add.s32 	%r121, %r11, %r153;
	mul.wide.s32 	%rd44, %r121, 4;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.nc.f32 	%f43, [%rd45];
	fma.rn.ftz.f32 	%f44, %f42, %f43, %f41;
	ld.global.nc.f32 	%f45, [%rd37+20];
	add.s32 	%r122, %r11, %r152;
	mul.wide.s32 	%rd46, %r122, 4;
	add.s64 	%rd47, %rd2, %rd46;
	ld.global.nc.f32 	%f46, [%rd47];
	fma.rn.ftz.f32 	%f82, %f45, %f46, %f44;
	add.s32 	%r157, %r157, 6;
	add.s32 	%r156, %r156, %r13;
	add.s32 	%r155, %r155, %r13;
	add.s32 	%r154, %r154, %r13;
	add.s32 	%r153, %r153, %r13;
	add.s32 	%r152, %r152, %r13;
	add.s64 	%rd75, %rd75, %rd4;
	setp.ne.s32 	%p2, %r157, 1536;
	@%p2 bra 	LBB1_4;
// %bb.5:                               // %"end for matrix_mul.s1.r4$x.1"
                                        //   in Loop: Header=BB1_1 Depth=1
	st.global.f32 	[%rd9], %f82;
	add.s32 	%r124, %r7, %r25;
	mul.wide.s32 	%rd48, %r124, 4;
	add.s64 	%rd12, %rd1, %rd48;
	ld.global.f32 	%f83, [%rd12];
	mov.u32 	%r163, 0;
	mov.u64 	%rd76, %rd77;
	mov.u32 	%r158, %r140;
	mov.u32 	%r159, %r141;
	mov.u32 	%r160, %r142;
	mov.u32 	%r161, %r143;
	mov.u32 	%r162, %r144;
LBB1_6:                                 // %"for matrix_mul.s1.r4$x.2"
                                        //   Parent Loop BB1_1 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	add.s32 	%r125, %r8, %r163;
	mul.wide.s32 	%rd49, %r125, 4;
	add.s64 	%rd50, %rd3, %rd49;
	ld.global.nc.f32 	%f47, [%rd50];
	ld.global.nc.f32 	%f48, [%rd76];
	fma.rn.ftz.f32 	%f49, %f47, %f48, %f83;
	ld.global.nc.f32 	%f50, [%rd50+4];
	add.s32 	%r126, %r11, %r162;
	mul.wide.s32 	%rd51, %r126, 4;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.nc.f32 	%f51, [%rd52];
	fma.rn.ftz.f32 	%f52, %f50, %f51, %f49;
	ld.global.nc.f32 	%f53, [%rd50+8];
	add.s32 	%r127, %r11, %r161;
	mul.wide.s32 	%rd53, %r127, 4;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.nc.f32 	%f54, [%rd54];
	fma.rn.ftz.f32 	%f55, %f53, %f54, %f52;
	ld.global.nc.f32 	%f56, [%rd50+12];
	add.s32 	%r128, %r11, %r160;
	mul.wide.s32 	%rd55, %r128, 4;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.nc.f32 	%f57, [%rd56];
	fma.rn.ftz.f32 	%f58, %f56, %f57, %f55;
	ld.global.nc.f32 	%f59, [%rd50+16];
	add.s32 	%r129, %r11, %r159;
	mul.wide.s32 	%rd57, %r129, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.nc.f32 	%f60, [%rd58];
	fma.rn.ftz.f32 	%f61, %f59, %f60, %f58;
	ld.global.nc.f32 	%f62, [%rd50+20];
	add.s32 	%r130, %r11, %r158;
	mul.wide.s32 	%rd59, %r130, 4;
	add.s64 	%rd60, %rd2, %rd59;
	ld.global.nc.f32 	%f63, [%rd60];
	fma.rn.ftz.f32 	%f83, %f62, %f63, %f61;
	add.s32 	%r163, %r163, 6;
	add.s32 	%r162, %r162, %r13;
	add.s32 	%r161, %r161, %r13;
	add.s32 	%r160, %r160, %r13;
	add.s32 	%r159, %r159, %r13;
	add.s32 	%r158, %r158, %r13;
	add.s64 	%rd76, %rd76, %rd4;
	setp.ne.s32 	%p3, %r163, 1536;
	@%p3 bra 	LBB1_6;
// %bb.7:                               // %"end for matrix_mul.s1.r4$x.2"
                                        //   in Loop: Header=BB1_1 Depth=1
	st.global.f32 	[%rd12], %f83;
	add.s32 	%r132, %r9, %r25;
	mul.wide.s32 	%rd61, %r132, 4;
	add.s64 	%rd15, %rd1, %rd61;
	ld.global.f32 	%f84, [%rd15];
	mov.u32 	%r169, 0;
	mov.u32 	%r164, %r140;
	mov.u32 	%r165, %r141;
	mov.u32 	%r166, %r142;
	mov.u32 	%r167, %r143;
	mov.u32 	%r168, %r144;
LBB1_8:                                 // %"for matrix_mul.s1.r4$x.3"
                                        //   Parent Loop BB1_1 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	add.s32 	%r133, %r10, %r169;
	mul.wide.s32 	%rd62, %r133, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.global.nc.f32 	%f64, [%rd63];
	ld.global.nc.f32 	%f65, [%rd77];
	fma.rn.ftz.f32 	%f66, %f64, %f65, %f84;
	ld.global.nc.f32 	%f67, [%rd63+4];
	add.s32 	%r134, %r11, %r168;
	mul.wide.s32 	%rd64, %r134, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.global.nc.f32 	%f68, [%rd65];
	fma.rn.ftz.f32 	%f69, %f67, %f68, %f66;
	ld.global.nc.f32 	%f70, [%rd63+8];
	add.s32 	%r135, %r11, %r167;
	mul.wide.s32 	%rd66, %r135, 4;
	add.s64 	%rd67, %rd2, %rd66;
	ld.global.nc.f32 	%f71, [%rd67];
	fma.rn.ftz.f32 	%f72, %f70, %f71, %f69;
	ld.global.nc.f32 	%f73, [%rd63+12];
	add.s32 	%r136, %r11, %r166;
	mul.wide.s32 	%rd68, %r136, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.global.nc.f32 	%f74, [%rd69];
	fma.rn.ftz.f32 	%f75, %f73, %f74, %f72;
	ld.global.nc.f32 	%f76, [%rd63+16];
	add.s32 	%r137, %r11, %r165;
	mul.wide.s32 	%rd70, %r137, 4;
	add.s64 	%rd71, %rd2, %rd70;
	ld.global.nc.f32 	%f77, [%rd71];
	fma.rn.ftz.f32 	%f78, %f76, %f77, %f75;
	ld.global.nc.f32 	%f79, [%rd63+20];
	add.s32 	%r138, %r11, %r164;
	mul.wide.s32 	%rd72, %r138, 4;
	add.s64 	%rd73, %rd2, %rd72;
	ld.global.nc.f32 	%f80, [%rd73];
	fma.rn.ftz.f32 	%f84, %f79, %f80, %f78;
	add.s32 	%r169, %r169, 6;
	add.s32 	%r168, %r168, %r13;
	add.s32 	%r167, %r167, %r13;
	add.s32 	%r166, %r166, %r13;
	add.s32 	%r165, %r165, %r13;
	add.s32 	%r164, %r164, %r13;
	add.s64 	%rd77, %rd77, %rd4;
	setp.ne.s32 	%p4, %r169, 1536;
	@%p4 bra 	LBB1_8;
// %bb.9:                               // %"end for matrix_mul.s1.r4$x.3"
                                        //   in Loop: Header=BB1_1 Depth=1
	st.global.f32 	[%rd15], %f84;
	add.s32 	%r145, %r145, 1;
	add.s32 	%r144, %r144, 1;
	add.s32 	%r143, %r143, 1;
	add.s32 	%r142, %r142, 1;
	add.s32 	%r141, %r141, 1;
	add.s32 	%r140, %r140, 1;
	add.s32 	%r139, %r139, 1;
	setp.eq.s32 	%p5, %r145, 8;
	@%p5 bra 	LBB1_10;
	bra.uni 	LBB1_1;
LBB1_10:                                // %"end for matrix_mul.s1.x.xi.xii"
	ret;
                                        // -- End function
}
	// .globl	kernel_output_s0_y_y___block_id_y // -- Begin function kernel_output_s0_y_y___block_id_y
.visible .entry kernel_output_s0_y_y___block_id_y(
	.param .u32 kernel_output_s0_y_y___block_id_y_param_0,
	.param .u64 kernel_output_s0_y_y___block_id_y_param_1,
	.param .u64 kernel_output_s0_y_y___block_id_y_param_2
)                                       // @kernel_output_s0_y_y___block_id_y
{
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<13>;

// %bb.0:                               // %entry
	ld.param.u32 	%r1, [kernel_output_s0_y_y___block_id_y_param_0];
	ld.param.u64 	%rd1, [kernel_output_s0_y_y___block_id_y_param_2];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.param.u64 	%rd3, [kernel_output_s0_y_y___block_id_y_param_1];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.y;
	mov.u32 	%r5, %tid.x;
	shl.b32 	%r6, %r3, 4;
	add.s32 	%r7, %r5, %r6;
	shl.b32 	%r8, %r2, 6;
	add.s32 	%r9, %r4, %r8;
	mad.lo.s32 	%r10, %r7, 768, %r9;
	shl.b32 	%r11, %r10, 1;
	mul.wide.u32 	%rd5, %r11, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u32 	%r12, [%rd6];
	shl.b32 	%r13, %r3, 3;
	mad.lo.s32 	%r14, %r9, %r1, %r13;
	shl.b32 	%r15, %r14, 1;
	add.s32 	%r16, %r15, %r5;
	mul.wide.s32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.u32 	[%rd8], %r12;
	mul.wide.s32 	%rd9, %r11, 4;
	add.s64 	%rd10, %rd4, %rd9;
	ld.global.nc.u32 	%r17, [%rd10+4];
	shl.b32 	%r18, %r9, 1;
	or.b32  	%r19, %r18, 1;
	mad.lo.s32 	%r20, %r19, %r1, %r7;
	mul.wide.s32 	%rd11, %r20, 4;
	add.s64 	%rd12, %rd2, %rd11;
	st.global.u32 	[%rd12], %r17;
	ret;
                                        // -- End function
}


add_temp_object_file: /tmp/Ouhq8L/mat_mul_classic_auto_schedule.a.o
Module.compile(): temporary object_name /tmp/Ouhq8L/mat_mul_classic_auto_schedule.a.o
emit_file.Compiling to native code...
Module.compile(): static_library_name ./bin/mat_mul_classic_auto_schedule.a
file_unlink: /tmp/Ouhq8L/mat_mul_classic_auto_schedule.a.o
dir_rmdir: /tmp/Ouhq8L
Module.compile(): assembly_name ./bin/mat_mul_classic_auto_schedule.s
emit_file.Compiling to native code...
Module.compile(): c_header_name ./bin/mat_mul_classic_auto_schedule.h
